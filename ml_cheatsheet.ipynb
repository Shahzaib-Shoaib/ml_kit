{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f9f21a-902e-4905-a09b-aeb72f7970bf",
   "metadata": {},
   "source": [
    "# Cheat Sheet: Key Steps and Decisions for MLP & NLP Mastery Challenge\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Initial Data Handling\n",
    "- **Load the dataset**:\n",
    "    - Use `pandas.read_csv()` or equivalent.\n",
    "    - **Check missing values**: `df.isnull().sum()`.\n",
    "\n",
    "- **Split your data**:\n",
    "    - Use `train_test_split` from `sklearn` (80/20 or 70/30 split).\n",
    "  \n",
    "- **Explore the dataset**:\n",
    "    - Use `df.describe()`, `df.info()`.\n",
    "    - Plot distributions with `sns.pairplot(df)` and `sns.heatmap(df.corr())`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Preprocessing\n",
    "- **Feature Scaling**:\n",
    "    - Use `StandardScaler` for normal distributions.\n",
    "    - Use `MinMaxScaler` or `RobustScaler` for non-normal distributions or data with outliers.\n",
    "\n",
    "- **Handle Missing Data**:\n",
    "    - **Numerical missing data**: `SimpleImputer`, `KNNImputer`.\n",
    "    - **Categorical missing data**: Use **Most Frequent Imputation** or add a new \"Missing\" category.\n",
    "\n",
    "- **Encoding Categorical Data**:\n",
    "    - Use `LabelEncoder` for binary categorical data.\n",
    "    - Use `OneHotEncoder` for multi-class categorical data.\n",
    "    - Use `ColumnTransformer` for applying transformations to specific columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Engineering\n",
    "- **Handle Date/Time Variables**:\n",
    "    - Extract day, month, year, etc.\n",
    "\n",
    "- **Feature Construction**:\n",
    "    - Create new features from existing ones (interaction terms, binning).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Outlier Detection & Removal\n",
    "- **Handle Outliers**:\n",
    "    - **Z-score** for normal data.\n",
    "    - **IQR Method** for non-normal data.\n",
    "    - **Winsorization** for capping outliers without removal.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Selection\n",
    "- **Algorithm choice**:\n",
    "    - **Classification**: Start with **Logistic Regression**, **KNN**, or **Random Forest**.\n",
    "    - **Regression**: Use **Linear Regression** or **Gradient Boosting**.\n",
    "    - **Dimensionality Reduction**: Use **PCA** if needed.\n",
    "\n",
    "- **Consider ensemble methods** like **Bagging**, **Boosting**, or **Stacking**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Training\n",
    "- **Fit the model**:\n",
    "    - `model.fit(X_train, y_train)`.\n",
    "\n",
    "- **Check for overfitting**:\n",
    "    - Use **cross-validation** (`cross_val_score`).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Evaluation\n",
    "- **For classification**:\n",
    "    - Evaluate with **accuracy**, **precision**, **recall**, **F1-score**, **confusion matrix**.\n",
    "    - For **imbalanced data**, use **F1-score** or **AUC-ROC curve**.\n",
    "\n",
    "- **For regression**:\n",
    "    - Evaluate using **MSE**, **RMSE**, **R2 Score**.\n",
    "  \n",
    "- **Plot performance**:\n",
    "    - Confusion matrices, ROC curves, and residual plots.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Hyperparameter Tuning\n",
    "- **Optimize parameters** using:\n",
    "    - **GridSearchCV** or **RandomizedSearchCV**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Post-Model Analysis\n",
    "- **Check feature importance**:\n",
    "    - Use `model.feature_importances_` for tree-based models like **Random Forest**.\n",
    "\n",
    "- **Evaluate model generalization**:\n",
    "    - Analyze test set performance and cross-validation scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Model Interpretability\n",
    "- **Interpret the model**:\n",
    "    - Use **SHAP** or **LIME** for interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Model Comparison\n",
    "- **Compare multiple models**:\n",
    "    - Train several models and compare their performance.\n",
    "    - Visualize model performance using bar charts for accuracy or relevant metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Final Notes\n",
    "- **Avoid common pitfalls**:\n",
    "    - Donâ€™t forget to **scale your data** (especially for distance-based algorithms).\n",
    "    - Always split your data into **train/test sets** to avoid data leakage.\n",
    "    - **Document your code** for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Algorithms to Remember:\n",
    "- **Logistic Regression** for binary classification.\n",
    "- **KNN** for non-parametric tasks.\n",
    "- **SVM** for complex decision boundaries.\n",
    "- **Random Forest** for feature importance.\n",
    "- **Gradient Boosting** for performance optimization.\n",
    "- **K-Means** for clustering.\n",
    "- **PCA** for dimensionality reduction.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e24a1-dd62-4c95-846c-bb271112c5f4",
   "metadata": {},
   "source": [
    " ## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f531fc-e211-47bc-97f3-f95af1fa9aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c542f4-368c-4966-bd26-6fab43742294",
   "metadata": {},
   "source": [
    "## 2. Reading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02d6b3-5013-4198-9c08-df1f710ac8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_dataset.csv' with your actual dataset filename\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Quick check for basic information\n",
    "df.info()  \n",
    "df.describe()  # Summary of numerical columns\n",
    "df.head()  # First few rows of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ff674-35b1-4e64-8279-1913eae655dc",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d3d49-d686-409f-bd5d-57ba0df4a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Fill missing numerical values using Simple Imputer (mean strategy)\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[['numerical_column']] = num_imputer.fit_transform(df[['numerical_column']])\n",
    "\n",
    "# Fill missing categorical values using the most frequent strategy\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['categorical_column']] = cat_imputer.fit_transform(df[['categorical_column']])\n",
    "\n",
    "# KNN Imputation (if needed)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = knn_imputer.fit_transform(df)\n",
    "\n",
    "# Drop rows/columns if necessary (optional)\n",
    "df.dropna(subset=['important_column'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16ea23-38e2-4502-b98c-77624c1f01f3",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46adb1a-456a-4ebb-82ed-fa6f70c0feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding (if categorical data is ordinal)\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_column'] = label_encoder.fit_transform(df['categorical_column'])\n",
    "\n",
    "# One-Hot Encoding (for nominal categorical data)\n",
    "df = pd.get_dummies(df, columns=['categorical_column'], drop_first=True)\n",
    "\n",
    "# Alternative: OneHotEncoder (if needed in a pipeline)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_data = one_hot_encoder.fit_transform(df[['categorical_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fbd99-5f79-46da-995b-28329da6be8f",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e53f3a-fca4-444b-920c-3304bfbf795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'target' is the column you want to predict\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Splitting into train and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c00637-2020-4071-b677-49dabb30fc05",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c79be0-e98e-405a-82a9-bc4bd070f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Normalization (MinMax Scaling)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_scaled = minmax_scaler.fit_transform(X_train)\n",
    "X_test_scaled = minmax_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17672e4-d92c-4a42-bc1b-10ff444848d2",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b29f2-fb16-4013-8f13-dc97175d4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score Method\n",
    "z_scores = np.abs((df - df.mean()) / df.std())\n",
    "df = df[(z_scores < 3).all(axis=1)]  # Removing rows with Z-scores > 3\n",
    "\n",
    "# IQR Method (Interquartile Range)\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df_cleaned = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df62de-89c9-46d0-94bd-3b04aa5fc7f1",
   "metadata": {},
   "source": [
    "## 8. Model Building (Basic Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb4d119-9af3-4771-9f35-a6aae27ffbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73672763-69be-4346-b47f-4f106949415c",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec0f13-c95d-4b61-9b69-4bc7403e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy Score\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033727d-77a0-438e-9d68-dbf103fe2bc8",
   "metadata": {},
   "source": [
    "## 10. Dealing with Imbalanced Data (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8e487-6b2b-4d94-876f-7c4c229f02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE for Oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93346b-17f6-4e6f-9d8c-22ed2a21ea6b",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b8966-819d-442f-b8be-bea10ca436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-validation accuracy scores: {cv_scores}')\n",
    "print(f'Mean CV accuracy: {cv_scores.mean()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b668a4-d5f7-418e-a982-bf41435ffa78",
   "metadata": {},
   "source": [
    "## 12. Pipelines (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b95b64-c026-442b-86f5-4644ca55a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example of a simple pipeline for scaling and modeling\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "print(f'Pipeline Accuracy: {accuracy_score(y_test, y_pred_pipeline)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139089a-c2a6-4fce-a6f1-bfce50ea3d8a",
   "metadata": {},
   "source": [
    "## 13. Handling Time and Date Variables (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6f6d3-f4d5-4da6-853e-156bc4dfff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Date to Year, Month, and Day features\n",
    "df['year'] = pd.to_datetime(df['date_column']).dt.year\n",
    "df['month'] = pd.to_datetime(df['date_column']).dt.month\n",
    "df['day'] = pd.to_datetime(df['date_column']).dt.day\n",
    "\n",
    "# Dropping the original date column (optional)\n",
    "df.drop('date_column', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7167f5-e41a-4951-9d8e-7db633ab39f6",
   "metadata": {},
   "source": [
    "## 14. Bias-Variance Tradeoff (Optional Explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc26c2c-f183-42e2-93d0-6fce73ff3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't a code snippet but a quick reminder:\n",
    "\n",
    "# Underfitting: High bias, low variance (simple models like linear regression)\n",
    "# Overfitting: Low bias, high variance (complex models like deep trees)\n",
    "\n",
    "# Use cross-validation and simpler models to reduce overfitting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60249899-a4b4-44b4-b933-bc8fc2651881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
