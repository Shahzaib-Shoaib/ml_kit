{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ce0fc-5b01-4391-b711-6d0e84c13f81",
   "metadata": {},
   "source": [
    "### Understanding Your Data\n",
    "\n",
    "Before starting any machine learning project, it's essential to understand the structure and characteristics of your data. This step helps you identify key features, data types, missing values, correlations, and any potential issues such as duplicates or outliers. Understanding your data allows you to make informed decisions for preprocessing and model building.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Shape & Overview**: Helps in getting a quick idea about the size of the dataset and the types of features it contains.\n",
    "- **Missing Values**: Identifies if there are missing values that need to be handled.\n",
    "- **Descriptive Statistics**: Summarizes basic statistical properties like mean, standard deviation, and percentiles.\n",
    "- **Correlation**: Shows relationships between features, useful for feature selection and detecting multicollinearity.\n",
    "- **Duplicates**: Identifies duplicate records that might need to be removed for better data quality.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Missing Data**: Be sure to handle missing values appropriately (impute or remove) to avoid data leakage.\n",
    "- **Outliers**: Investigate whether the dataset has outliers that need to be treated.\n",
    "- **Data Types**: Ensure that your features have the correct data types (e.g., categorical, numerical) for further processing.\n",
    "- **Duplicate Data**: Always check for duplicate rows to avoid skewing your results.\n",
    "- **Correlations**: Highly correlated features can lead to multicollinearity, which may degrade model performance.\n",
    "\n",
    "---\n",
    "\n",
    "By performing these checks, you gain a solid understanding of the data and can proceed with preprocessing and feature engineering confidently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa9e74a-a009-4c8d-aa1f-c88389c37126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Understanding Your Data\n",
    "\n",
    "# Get the shape of the dataset (rows, columns)\n",
    "df.shape\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Randomly sample 5 rows from the dataset\n",
    "df.sample(5)\n",
    "\n",
    "# Get concise information about data types and non-null values\n",
    "df.info()\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "df.describe()\n",
    "\n",
    "# Check for duplicate rows\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Display correlation matrix between numerical columns\n",
    "df.corr()\n",
    "\n",
    "# Correlation of a specific column with others (e.g., 'target_column')\n",
    "df.corr()['target_column']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e65c-d6ac-4f3f-a9f2-4c581dbcf304",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of investigating datasets to discover patterns, spot anomalies, test hypotheses, and check assumptions using summary statistics and graphical representations. EDA is a crucial step in understanding the distribution and relationships between variables, preparing the data for modeling, and identifying features that may require transformation.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Data Type Identification**: Helps categorize features as numerical, categorical, or mixed.\n",
    "- **Skewness**: Understands if a distribution is skewed, which may affect model performance.\n",
    "- **Univariate & Bivariate Analysis**: Examines single-variable distributions and relationships between pairs of variables.\n",
    "- **Correlation and Interactions**: Detects multicollinearity and relationships between multiple variables using heatmaps, boxplots, and KDE (Kernel Density Estimation) plots.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Data Types (Numerical, Categorical, Mixed)**:\n",
    "   - Identifying feature types helps guide which visualizations and transformations to apply.\n",
    "   \n",
    "2. **Skewness**:\n",
    "   - Skewness checks are important for understanding the distribution shape of numerical data.\n",
    "   - Skewed data may require transformations like log or power transformations.\n",
    "\n",
    "3. **Univariate Analysis**:\n",
    "   - Looks at the distribution of a single variable. Histograms or KDE plots are used for numerical data, and bar plots for categorical data.\n",
    "\n",
    "4. **Bivariate Analysis**:\n",
    "   - Examines relationships between two variables using scatterplots, boxplots, and 2D KDE plots for numerical data.\n",
    "\n",
    "5. **2D KDE Plot**:\n",
    "   - Visualizes the bivariate distribution of two numerical variables, providing insight into their joint density.\n",
    "\n",
    "6. **Boxplot**:\n",
    "   - Useful for understanding the distribution of numerical data and identifying outliers.\n",
    "\n",
    "7. **Bar Plot**:\n",
    "   - Displays the frequency of categorical variables.\n",
    "\n",
    "8. **Heatmap**:\n",
    "   - Useful for visualizing correlations between multiple numerical features, helping detect multicollinearity.\n",
    "\n",
    "9. **Cross Tabulation (pd.crosstab)**:\n",
    "   - Compares the frequency distribution between two categorical variables.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Skewness**: Highly skewed data may require transformations.\n",
    "- **Outliers**: Boxplots help detect outliers, which may need special handling.\n",
    "- **Multicollinearity**: Heatmaps help in detecting highly correlated features, which might need to be removed.\n",
    "- **Mixed Variables**: Features with mixed data types (numerical & categorical) require special handling, such as binning for continuous data.\n",
    "\n",
    "---\n",
    "\n",
    "Performing EDA helps you better understand the data, decide on feature engineering steps, and improve the quality of inputs for your machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02524b38-b5cc-4866-986f-3bf3d04fcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for EDA\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Types (Numerical, Categorical)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Skewness\n",
    "print(df[numerical_columns].skew())\n",
    "\n",
    "# Univariate Analysis (Histogram for Numerical Columns)\n",
    "df[numerical_columns].hist(figsize=(10, 8))\n",
    "\n",
    "# Bar Plot for Categorical Data\n",
    "df[categorical_columns[0]].value_counts().plot(kind='bar')\n",
    "\n",
    "# Bivariate Analysis (2D KDE Plot)\n",
    "sns.kdeplot(x='num_col1', y='num_col2', data=df, cmap='coolwarm', shade=True)\n",
    "\n",
    "# Boxplot for Numerical and Categorical Data\n",
    "sns.boxplot(x='categorical_column', y='numerical_column', data=df)\n",
    "\n",
    "# Heatmap of Correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Cross Tabulation\n",
    "pd.crosstab(df['categorical_col1'], df['categorical_col2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfbd97-22db-41ff-82d3-547ffa86ffc3",
   "metadata": {},
   "source": [
    "### What is Feature Engineering?\n",
    "\n",
    "Feature Engineering is the process of using domain knowledge to extract or create new input features from raw data, improving the performance of machine learning models. It involves transforming data into formats that are better suited for algorithms, and it can greatly impact the accuracy and performance of the models.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Feature Scaling**: Adjusts numerical features to a standard scale, which is crucial for algorithms sensitive to feature magnitude, such as distance-based algorithms (e.g., k-NN, SVM, etc.).\n",
    "- **Feature Construction**: Involves creating new features by combining existing ones. This can capture important patterns that aren't explicitly present in the raw data.\n",
    "- **Feature Selection**: Reduces the dimensionality of the dataset by selecting the most important features, which can help improve model performance and reduce overfitting.\n",
    "- **Feature Extraction**: Derives new features from raw data through techniques like PCA, which can reduce redundancy in the data and highlight important patterns.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Feature Scaling**:\n",
    "   - Ensures that numerical data is on the same scale, which helps machine learning models perform better.\n",
    "   - Common techniques include:\n",
    "     - **Standardization**: Rescales features to have zero mean and unit variance.\n",
    "     - **Normalization**: Rescales features to a range of 0-1 using Min-Max scaling.\n",
    "\n",
    "2. **Feature Construction**:\n",
    "   - Combines or transforms existing features to create new ones, such as adding interaction terms, binning continuous data, or aggregating features.\n",
    "   - Example: Constructing \"total_expense\" from \"monthly_expense\" and \"number_of_months\".\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - Identifies the most important features for the model, reducing overfitting and improving generalization.\n",
    "   - Techniques include:\n",
    "     - **Correlation-based**: Removes highly correlated features.\n",
    "     - **Wrapper methods**: Evaluates feature subsets by training models.\n",
    "     - **Embedded methods**: Uses algorithms like Lasso to perform feature selection.\n",
    "\n",
    "4. **Feature Extraction**:\n",
    "   - Reduces dimensionality by extracting features that encapsulate the most variance in the data. Principal Component Analysis (PCA) is commonly used here.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Scaling**: Most ML algorithms assume features are on the same scale, especially distance-based models.\n",
    "- **Construction**: Ensure that the constructed features are meaningful and capture the right relationships.\n",
    "- **Selection**: Avoid using too many or too few features, as this can lead to overfitting or underfitting.\n",
    "- **Extraction**: Feature extraction can help in high-dimensional datasets where reducing the number of features is critical.\n",
    "\n",
    "---\n",
    "\n",
    "Proper feature engineering helps models learn from data more effectively, leading to improved predictions and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dac0d-2e49-4623-9d8a-cec4e95b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "normalized_data = minmax_scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Feature Construction (Creating a new feature from existing ones)\n",
    "df['total_expense'] = df['monthly_expense'] * df['number_of_months']\n",
    "\n",
    "# Feature Selection (Select K Best based on ANOVA F-value)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "selected_features = selector.fit_transform(df[numerical_columns], df['target'])\n",
    "\n",
    "# Feature Extraction (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e81d6-2373-4fbc-8370-0918b640d5f6",
   "metadata": {},
   "source": [
    "### Pandas Code Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350b0fb-3184-47ee-b05a-58c3d471c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values directly in the original DataFrame\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Fill missing values with 0 in the original DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Drop a specific column 'age' from the DataFrame\n",
    "df.drop('age', axis=1, inplace=True)\n",
    "\n",
    "# Sort the DataFrame by the 'salary' column\n",
    "df.sort_values(by='salary', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56c9ef-a5ef-485b-96e6-d6df900e9dcc",
   "metadata": {},
   "source": [
    "### Pandas Profiling\n",
    "\n",
    "Pandas Profiling is a powerful tool for automating Exploratory Data Analysis (EDA). It quickly generates a comprehensive report of your dataset, summarizing key statistics and identifying potential data quality issues. This tool is especially useful for getting an overview of the dataset, such as descriptive statistics, correlations, missing data, and data types, without manually writing multiple code blocks. It provides detailed visualizations of distributions, correlations, and interactions, saving time and effort during initial data exploration.\n",
    "\n",
    "**Why Use It?**  \n",
    "- Provides a **quick EDA** summary of the dataset, offering detailed insights into data distributions, missing values, correlations, and feature statistics.\n",
    "- Highlights data quality issues such as missing values, outliers, high cardinality, and duplicates.\n",
    "- **Efficient for initial data exploration**, making it easier to understand the structure and issues of the dataset before feature engineering or model building.\n",
    "  \n",
    "**Key Features**:\n",
    "- Overview of dataset, including total missing values, duplicates, and feature cardinality.\n",
    "- Visual summaries of **univariate distributions**, correlations, and missing data patterns.\n",
    "- Flags **warnings** for potential issues in the dataset, helping you detect outliers or problematic data.\n",
    "  \n",
    "**Keep in Mind**:\n",
    "- For large datasets, generating a profiling report can be slow.\n",
    "- Use this as an early step in data exploration to gain insights into data quality and potential areas for feature engineering or cleaning.\n",
    "- Be mindful of the report's sensitivity when working with private or sensitive data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a824a-d130-4d58-a9ad-60a78f76a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas profiling if it's not installed\n",
    "# pip install pandas-profiling\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "# Generate a pandas profiling report for your DataFrame\n",
    "profile = df.profile_report(title=\"Pandas Profiling Report\")\n",
    "\n",
    "# Display the report in a Jupyter Notebook\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "# Optionally, save the report to an HTML file for further analysis\n",
    "profile.to_file(\"output_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4bb23-ad4a-44fa-adc2-9cb82ef92bc2",
   "metadata": {},
   "source": [
    "### Feature Scaling - Standardization\n",
    "\n",
    "**Standardization** (also known as Z-score normalization or mean centering) is a scaling technique where the features of the data are transformed such that they have a **mean of 0** and a **standard deviation of 1**. This is especially important for algorithms that are sensitive to the scale of the data, as it ensures that all features contribute equally during model training.\n",
    "\n",
    "**Why Use It?**  \n",
    "Some machine learning algorithms are sensitive to the scale of features, and it is important to normalize the range of all features to ensure they have comparable contributions to the model. Algorithms like **KMeans**, **K Nearest Neighbours (KNN)**, **PCA (Principal Component Analysis)**, and **Artificial Neural Networks** are particularly sensitive to the scales of the input data.\n",
    "\n",
    "**Use Cases**:\n",
    "- **KMeans Clustering**: Distance-based algorithms like KMeans require features to be on the same scale.\n",
    "- **KNN (K-Nearest Neighbors)**: For distance calculations in KNN, all features should be scaled appropriately.\n",
    "- **PCA**: Ensures that the principal components are influenced equally by all features.\n",
    "- **Neural Networks**: Helps speed up training and avoids certain features dominating the learning process due to larger magnitude.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Standardization works well with normally distributed features.\n",
    "- It can sometimes distort the importance of features that do not need to be scaled. Only use it for models that require features to be on the same scale.\n",
    "- Apply **standardization** only on the training data, and later use the same scaling factors for the test data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same transformation on test data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1a2d3-9485-4cef-984d-43fc688a0392",
   "metadata": {},
   "source": [
    "### Feature Scaling - Normalization | MinMaxScaling | MaxAbsScaling | RobustScaling\n",
    "\n",
    "**Normalization** involves rescaling the features of your dataset to a specific range, typically [0, 1] or [-1, 1], depending on the scaling technique. This is essential for machine learning models that are sensitive to the magnitude of input features.\n",
    "\n",
    "**Why Use It?**  \n",
    "Many machine learning algorithms like **logistic regression**, **support vector machines (SVM)**, and **neural networks** can perform better when features are on the same scale. Different scaling techniques suit different types of data and tasks.\n",
    "\n",
    "**Types of Scaling**:\n",
    "- **MinMaxScaling**: Scales features to a given range, usually [0, 1]. Best suited for algorithms that assume the features are bounded within a specific range.\n",
    "- **MaxAbsScaling**: Scales features by their maximum absolute value, leaving the sign unchanged. Ideal for data that is already centered at zero but needs rescaling.\n",
    "- **RobustScaling**: Uses the median and interquartile range (IQR) for scaling, making it more robust to outliers compared to MinMaxScaling.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- **Fit** the scaler on the **training data only** to avoid data leakage.\n",
    "- **Transform** both training and test datasets with the same scaler to ensure consistency.\n",
    "- MinMaxScaling can be sensitive to outliers, while RobustScaling handles them better.\n",
    "  \n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "min_max_scaler = MinMaxScaler()\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit on the training data only and transform both train and test data\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9657e6-e69f-42ad-a74a-816625bba7b9",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data | Ordinal Encoding | Label Encoding\n",
    "\n",
    "**Categorical Encoding** is essential when dealing with features that contain categorical data, as most machine learning algorithms expect numerical input. Two common types of encoding are **Ordinal Encoding** and **Label Encoding**.\n",
    "\n",
    "**Why Use It?**\n",
    "Machine learning algorithms cannot handle raw categorical data, and encoding is necessary to convert these categories into numerical values that models can understand.\n",
    "\n",
    "#### Types of Encoding:\n",
    "- **Ordinal Encoding**: This encoding is used when the categorical variable has an inherent order (e.g., \"low\", \"medium\", \"high\"). Each category is assigned an integer based on the rank.\n",
    "- **Label Encoding**: Assigns a unique integer to each category, with no inherent order. This is useful for nominal data, where no ordering exists.\n",
    "\n",
    "**Use Cases**:\n",
    "- **Ordinal Encoding** is suitable for ordinal features with meaningful rankings.\n",
    "- **Label Encoding** is generally used for nominal features (without order) but can introduce issues with algorithms that assume some ordering from integer values.\n",
    "\n",
    "**Column Transformer**: This is used when you need to apply different preprocessing techniques (like encoding or scaling) to different columns of your dataset. It is especially useful for pipelines with both categorical and numerical features.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Ordinal encoding should only be applied when the order of categories matters.\n",
    "- Label encoding can mislead some models if there is no inherent ordering but numbers are assigned. Consider **One-Hot Encoding** for such cases.\n",
    "- Fit the encoder on the training data and transform both train and test datasets.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "X = [['low'], ['medium'], ['high']]\n",
    "\n",
    "# Ordinal Encoding (for ordinal features)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "X_ordinal_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# Label Encoding (for nominal features)\n",
    "label_encoder = LabelEncoder()\n",
    "y = ['cat', 'dog', 'mouse']\n",
    "y_label_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Column Transformer - Example (if needed)\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(), ['categorical_column']),\n",
    "    ('num', 'passthrough', ['numerical_column'])\n",
    "])\n",
    "\n",
    "# Apply the transformations to the dataset\n",
    "X_transformed = column_transformer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42139ed6-d455-4d88-a308-1763bfd83b60",
   "metadata": {},
   "source": [
    "### One Hot Encoding | Handling Categorical Data\n",
    "\n",
    "**One-Hot Encoding** is a technique used to convert categorical data into a binary (0 or 1) matrix format. Each unique category is transformed into a separate column, where the presence of a category is marked as 1 and the absence as 0.\n",
    "\n",
    "**Why Use It?**  \n",
    "Unlike **Label Encoding**, which can introduce a false ordinal relationship between categories, One-Hot Encoding ensures that no such ordering is implied. It's particularly useful for nominal categorical data where categories have no order.\n",
    "\n",
    "#### Multicollinearity Concern:\n",
    "- **Multicollinearity** can arise when the columns created by one-hot encoding are not independent. To avoid this, we drop one column from the set of n categories (thus creating n-1 columns). This helps avoid the **Dummy Variable Trap**, where the presence of a category is perfectly predicted by the other categories.\n",
    "\n",
    "**Key Points to Keep in Mind**:\n",
    "- Use **One-Hot Encoding** when categories are **nominal** and there's no natural order.\n",
    "- Always ensure input columns are **independent** after encoding.\n",
    "- For categorical features with n categories, One-Hot Encoding will create **n-1 columns** to avoid multicollinearity.\n",
    "- Avoid the dummy variable trap by dropping one category from the encoded matrix.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi']\n",
    "})\n",
    "\n",
    "# One-Hot Encoding using pandas\n",
    "# We use drop_first=True to avoid the Dummy Variable Trap (n-1 columns)\n",
    "df_encoded = pd.get_dummies(df, columns=['City'], drop_first=True)\n",
    "\n",
    "# Example output would be:\n",
    "#    City_Lahore  City_Islamabad  City_Karachi\n",
    "# 0            0               0             1\n",
    "# 1            1               0             0\n",
    "# 2            0               1             0\n",
    "# 3            0               0             1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a32bb-e303-42a0-8973-e51f45d40cb4",
   "metadata": {},
   "source": [
    "### Column Transformer in Machine Learning\n",
    "\n",
    "The **Column Transformer** is a useful tool in machine learning when you need to apply different preprocessing techniques to different types of features (columns). For instance, you might want to apply **scaling** to numerical data while applying **encoding** to categorical data simultaneously.\n",
    "\n",
    "**Why Use It?**  \n",
    "It streamlines the preprocessing of datasets that contain both numerical and categorical variables, enabling you to handle each column type appropriately without manually applying transformations to each column.\n",
    "\n",
    "#### Key Use Cases:\n",
    "- **Numerical columns**: You may want to scale or normalize these features.\n",
    "- **Categorical columns**: You can apply encoders such as **One-Hot Encoding** or **Ordinal Encoding**.\n",
    "- **Pipelines**: The Column Transformer works well in conjunction with machine learning pipelines for streamlined workflows.\n",
    "\n",
    "#### Important Points to Consider:\n",
    "- Ensure the right transformation is applied to the correct column type.\n",
    "- You can chain transformers together to process different columns efficiently.\n",
    "- It's essential to fit the column transformer on training data and then apply it to both training and test sets to avoid **data leakage**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 32, 40, 50],\n",
    "    'city': ['Karachi', 'Lahore', 'Islamabad', 'Lahore'],\n",
    "    'salary': [50000, 60000, 65000, 70000]\n",
    "})\n",
    "\n",
    "# Column Transformer setup\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), ['age', 'salary']),  # Scaling numerical data\n",
    "    ('cat', OneHotEncoder(drop='first'), ['city'])  # One-Hot Encoding categorical data\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = column_transformer.fit_transform(df)\n",
    "\n",
    "# The numerical columns are scaled, and the categorical column is one-hot encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77278-f3ec-4544-9e86-71bf03cd87ed",
   "metadata": {},
   "source": [
    "### Function Transformer | Log Transform | Reciprocal Transform | Square Root Transform\n",
    "\n",
    "**Function Transformers** allow you to apply mathematical transformations to your features. Some of the common transformations include:\n",
    "- **Log Transform**: Helps handle skewed data by reducing the effect of large outliers. Used when data is highly right-skewed.\n",
    "- **Reciprocal Transform**: Similar to log transform but stronger. Works well with extreme values.\n",
    "- **Square Root Transform**: Less aggressive than log, used for moderately skewed data.\n",
    "\n",
    "These transformations are typically used when you want to make non-normally distributed data more Gaussian (normal), which helps improve model performance for algorithms that assume normality (e.g., linear regression, SVM).\n",
    "\n",
    "#### How to Check if Distribution is Normal?\n",
    "\n",
    "1. **sns.distplot()**: This plots the distribution of your data.\n",
    "   - If the plot shows a bell-shaped curve, the data is normally distributed.\n",
    "   \n",
    "2. **pd.skew()**: Returns the skewness of the data. \n",
    "   - If the skew is close to 0, the data is symmetric. \n",
    "   - Positive values indicate right-skewed data, and negative values indicate left-skewed data.\n",
    "   \n",
    "3. **QQ Plot**: (Quantile-Quantile plot) checks the normality by comparing the quantiles of your data against a theoretical normal distribution.\n",
    "   - If the points lie on the diagonal line, the data is normally distributed.\n",
    "\n",
    "#### Key Points:\n",
    "- **Log Transform**: Use for right-skewed data.\n",
    "- **Reciprocal Transform**: Use for data with extreme values.\n",
    "- **Square Root Transform**: Use for moderately skewed data.\n",
    "- Checking the data's distribution is crucial before applying transformations.\n",
    "- After transforming, always recheck the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'income': [1000, 3000, 10000, 50000, 100000, 200000]\n",
    "})\n",
    "\n",
    "# Checking Distribution with Seaborn's distplot\n",
    "sns.distplot(df['income'])\n",
    "plt.show()\n",
    "\n",
    "# Checking Skewness\n",
    "print(f\"Skewness: {df['income'].skew()}\")\n",
    "\n",
    "# QQ Plot to check normality\n",
    "stats.probplot(df['income'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# Applying Log Transform to reduce skewness\n",
    "df['income_log'] = np.log(df['income'] + 1)\n",
    "\n",
    "# Checking the transformed distribution\n",
    "sns.distplot(df['income_log'])\n",
    "plt.show()\n",
    "\n",
    "# Checking skewness after transformation\n",
    "print(f\"Skewness after log transform: {df['income_log'].skew()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f760847-ee02-408f-b66f-ae3dbd7d5769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
