{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ce0fc-5b01-4391-b711-6d0e84c13f81",
   "metadata": {},
   "source": [
    "### Understanding Your Data\n",
    "\n",
    "Before starting any machine learning project, it's essential to understand the structure and characteristics of your data. This step helps you identify key features, data types, missing values, correlations, and any potential issues such as duplicates or outliers. Understanding your data allows you to make informed decisions for preprocessing and model building.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Shape & Overview**: Helps in getting a quick idea about the size of the dataset and the types of features it contains.\n",
    "- **Missing Values**: Identifies if there are missing values that need to be handled.\n",
    "- **Descriptive Statistics**: Summarizes basic statistical properties like mean, standard deviation, and percentiles.\n",
    "- **Correlation**: Shows relationships between features, useful for feature selection and detecting multicollinearity.\n",
    "- **Duplicates**: Identifies duplicate records that might need to be removed for better data quality.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Missing Data**: Be sure to handle missing values appropriately (impute or remove) to avoid data leakage.\n",
    "- **Outliers**: Investigate whether the dataset has outliers that need to be treated.\n",
    "- **Data Types**: Ensure that your features have the correct data types (e.g., categorical, numerical) for further processing.\n",
    "- **Duplicate Data**: Always check for duplicate rows to avoid skewing your results.\n",
    "- **Correlations**: Highly correlated features can lead to multicollinearity, which may degrade model performance.\n",
    "\n",
    "---\n",
    "\n",
    "By performing these checks, you gain a solid understanding of the data and can proceed with preprocessing and feature engineering confidently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa9e74a-a009-4c8d-aa1f-c88389c37126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Understanding Your Data\n",
    "\n",
    "# Get the shape of the dataset (rows, columns)\n",
    "df.shape\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Randomly sample 5 rows from the dataset\n",
    "df.sample(5)\n",
    "\n",
    "# Get concise information about data types and non-null values\n",
    "df.info()\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "df.describe()\n",
    "\n",
    "# Check for duplicate rows\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Display correlation matrix between numerical columns\n",
    "df.corr()\n",
    "\n",
    "# Correlation of a specific column with others (e.g., 'target_column')\n",
    "df.corr()['target_column']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e65c-d6ac-4f3f-a9f2-4c581dbcf304",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of investigating datasets to discover patterns, spot anomalies, test hypotheses, and check assumptions using summary statistics and graphical representations. EDA is a crucial step in understanding the distribution and relationships between variables, preparing the data for modeling, and identifying features that may require transformation.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Data Type Identification**: Helps categorize features as numerical, categorical, or mixed.\n",
    "- **Skewness**: Understands if a distribution is skewed, which may affect model performance.\n",
    "- **Univariate & Bivariate Analysis**: Examines single-variable distributions and relationships between pairs of variables.\n",
    "- **Correlation and Interactions**: Detects multicollinearity and relationships between multiple variables using heatmaps, boxplots, and KDE (Kernel Density Estimation) plots.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Data Types (Numerical, Categorical, Mixed)**:\n",
    "   - Identifying feature types helps guide which visualizations and transformations to apply.\n",
    "   \n",
    "2. **Skewness**:\n",
    "   - Skewness checks are important for understanding the distribution shape of numerical data.\n",
    "   - Skewed data may require transformations like log or power transformations.\n",
    "\n",
    "3. **Univariate Analysis**:\n",
    "   - Looks at the distribution of a single variable. Histograms or KDE plots are used for numerical data, and bar plots for categorical data.\n",
    "\n",
    "4. **Bivariate Analysis**:\n",
    "   - Examines relationships between two variables using scatterplots, boxplots, and 2D KDE plots for numerical data.\n",
    "\n",
    "5. **2D KDE Plot**:\n",
    "   - Visualizes the bivariate distribution of two numerical variables, providing insight into their joint density.\n",
    "\n",
    "6. **Boxplot**:\n",
    "   - Useful for understanding the distribution of numerical data and identifying outliers.\n",
    "\n",
    "7. **Bar Plot**:\n",
    "   - Displays the frequency of categorical variables.\n",
    "\n",
    "8. **Heatmap**:\n",
    "   - Useful for visualizing correlations between multiple numerical features, helping detect multicollinearity.\n",
    "\n",
    "9. **Cross Tabulation (pd.crosstab)**:\n",
    "   - Compares the frequency distribution between two categorical variables.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Skewness**: Highly skewed data may require transformations.\n",
    "- **Outliers**: Boxplots help detect outliers, which may need special handling.\n",
    "- **Multicollinearity**: Heatmaps help in detecting highly correlated features, which might need to be removed.\n",
    "- **Mixed Variables**: Features with mixed data types (numerical & categorical) require special handling, such as binning for continuous data.\n",
    "\n",
    "---\n",
    "\n",
    "Performing EDA helps you better understand the data, decide on feature engineering steps, and improve the quality of inputs for your machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02524b38-b5cc-4866-986f-3bf3d04fcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for EDA\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Types (Numerical, Categorical)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Skewness\n",
    "print(df[numerical_columns].skew())\n",
    "\n",
    "# Univariate Analysis (Histogram for Numerical Columns)\n",
    "df[numerical_columns].hist(figsize=(10, 8))\n",
    "\n",
    "# Bar Plot for Categorical Data\n",
    "df[categorical_columns[0]].value_counts().plot(kind='bar')\n",
    "\n",
    "# Bivariate Analysis (2D KDE Plot)\n",
    "sns.kdeplot(x='num_col1', y='num_col2', data=df, cmap='coolwarm', shade=True)\n",
    "\n",
    "# Boxplot for Numerical and Categorical Data\n",
    "sns.boxplot(x='categorical_column', y='numerical_column', data=df)\n",
    "\n",
    "# Heatmap of Correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Cross Tabulation\n",
    "pd.crosstab(df['categorical_col1'], df['categorical_col2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfbd97-22db-41ff-82d3-547ffa86ffc3",
   "metadata": {},
   "source": [
    "### What is Feature Engineering?\n",
    "\n",
    "Feature Engineering is the process of using domain knowledge to extract or create new input features from raw data, improving the performance of machine learning models. It involves transforming data into formats that are better suited for algorithms, and it can greatly impact the accuracy and performance of the models.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Feature Scaling**: Adjusts numerical features to a standard scale, which is crucial for algorithms sensitive to feature magnitude, such as distance-based algorithms (e.g., k-NN, SVM, etc.).\n",
    "- **Feature Construction**: Involves creating new features by combining existing ones. This can capture important patterns that aren't explicitly present in the raw data.\n",
    "- **Feature Selection**: Reduces the dimensionality of the dataset by selecting the most important features, which can help improve model performance and reduce overfitting.\n",
    "- **Feature Extraction**: Derives new features from raw data through techniques like PCA, which can reduce redundancy in the data and highlight important patterns.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Feature Scaling**:\n",
    "   - Ensures that numerical data is on the same scale, which helps machine learning models perform better.\n",
    "   - Common techniques include:\n",
    "     - **Standardization**: Rescales features to have zero mean and unit variance.\n",
    "     - **Normalization**: Rescales features to a range of 0-1 using Min-Max scaling.\n",
    "\n",
    "2. **Feature Construction**:\n",
    "   - Combines or transforms existing features to create new ones, such as adding interaction terms, binning continuous data, or aggregating features.\n",
    "   - Example: Constructing \"total_expense\" from \"monthly_expense\" and \"number_of_months\".\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - Identifies the most important features for the model, reducing overfitting and improving generalization.\n",
    "   - Techniques include:\n",
    "     - **Correlation-based**: Removes highly correlated features.\n",
    "     - **Wrapper methods**: Evaluates feature subsets by training models.\n",
    "     - **Embedded methods**: Uses algorithms like Lasso to perform feature selection.\n",
    "\n",
    "4. **Feature Extraction**:\n",
    "   - Reduces dimensionality by extracting features that encapsulate the most variance in the data. Principal Component Analysis (PCA) is commonly used here.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Scaling**: Most ML algorithms assume features are on the same scale, especially distance-based models.\n",
    "- **Construction**: Ensure that the constructed features are meaningful and capture the right relationships.\n",
    "- **Selection**: Avoid using too many or too few features, as this can lead to overfitting or underfitting.\n",
    "- **Extraction**: Feature extraction can help in high-dimensional datasets where reducing the number of features is critical.\n",
    "\n",
    "---\n",
    "\n",
    "Proper feature engineering helps models learn from data more effectively, leading to improved predictions and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dac0d-2e49-4623-9d8a-cec4e95b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "normalized_data = minmax_scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Feature Construction (Creating a new feature from existing ones)\n",
    "df['total_expense'] = df['monthly_expense'] * df['number_of_months']\n",
    "\n",
    "# Feature Selection (Select K Best based on ANOVA F-value)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "selected_features = selector.fit_transform(df[numerical_columns], df['target'])\n",
    "\n",
    "# Feature Extraction (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e81d6-2373-4fbc-8370-0918b640d5f6",
   "metadata": {},
   "source": [
    "### Pandas Code Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350b0fb-3184-47ee-b05a-58c3d471c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values directly in the original DataFrame\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Fill missing values with 0 in the original DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Drop a specific column 'age' from the DataFrame\n",
    "df.drop('age', axis=1, inplace=True)\n",
    "\n",
    "# Sort the DataFrame by the 'salary' column\n",
    "df.sort_values(by='salary', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56c9ef-a5ef-485b-96e6-d6df900e9dcc",
   "metadata": {},
   "source": [
    "### Pandas Profiling\n",
    "\n",
    "Pandas Profiling is a powerful tool for automating Exploratory Data Analysis (EDA). It quickly generates a comprehensive report of your dataset, summarizing key statistics and identifying potential data quality issues. This tool is especially useful for getting an overview of the dataset, such as descriptive statistics, correlations, missing data, and data types, without manually writing multiple code blocks. It provides detailed visualizations of distributions, correlations, and interactions, saving time and effort during initial data exploration.\n",
    "\n",
    "**Why Use It?**  \n",
    "- Provides a **quick EDA** summary of the dataset, offering detailed insights into data distributions, missing values, correlations, and feature statistics.\n",
    "- Highlights data quality issues such as missing values, outliers, high cardinality, and duplicates.\n",
    "- **Efficient for initial data exploration**, making it easier to understand the structure and issues of the dataset before feature engineering or model building.\n",
    "  \n",
    "**Key Features**:\n",
    "- Overview of dataset, including total missing values, duplicates, and feature cardinality.\n",
    "- Visual summaries of **univariate distributions**, correlations, and missing data patterns.\n",
    "- Flags **warnings** for potential issues in the dataset, helping you detect outliers or problematic data.\n",
    "  \n",
    "**Keep in Mind**:\n",
    "- For large datasets, generating a profiling report can be slow.\n",
    "- Use this as an early step in data exploration to gain insights into data quality and potential areas for feature engineering or cleaning.\n",
    "- Be mindful of the report's sensitivity when working with private or sensitive data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a824a-d130-4d58-a9ad-60a78f76a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas profiling if it's not installed\n",
    "# pip install pandas-profiling\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "# Generate a pandas profiling report for your DataFrame\n",
    "profile = df.profile_report(title=\"Pandas Profiling Report\")\n",
    "\n",
    "# Display the report in a Jupyter Notebook\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "# Optionally, save the report to an HTML file for further analysis\n",
    "profile.to_file(\"output_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4bb23-ad4a-44fa-adc2-9cb82ef92bc2",
   "metadata": {},
   "source": [
    "### Feature Scaling - Standardization\n",
    "\n",
    "**Standardization** (also known as Z-score normalization or mean centering) is a scaling technique where the features of the data are transformed such that they have a **mean of 0** and a **standard deviation of 1**. This is especially important for algorithms that are sensitive to the scale of the data, as it ensures that all features contribute equally during model training.\n",
    "\n",
    "**Why Use It?**  \n",
    "Some machine learning algorithms are sensitive to the scale of features, and it is important to normalize the range of all features to ensure they have comparable contributions to the model. Algorithms like **KMeans**, **K Nearest Neighbours (KNN)**, **PCA (Principal Component Analysis)**, and **Artificial Neural Networks** are particularly sensitive to the scales of the input data.\n",
    "\n",
    "**Use Cases**:\n",
    "- **KMeans Clustering**: Distance-based algorithms like KMeans require features to be on the same scale.\n",
    "- **KNN (K-Nearest Neighbors)**: For distance calculations in KNN, all features should be scaled appropriately.\n",
    "- **PCA**: Ensures that the principal components are influenced equally by all features.\n",
    "- **Neural Networks**: Helps speed up training and avoids certain features dominating the learning process due to larger magnitude.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Standardization works well with normally distributed features.\n",
    "- It can sometimes distort the importance of features that do not need to be scaled. Only use it for models that require features to be on the same scale.\n",
    "- Apply **standardization** only on the training data, and later use the same scaling factors for the test data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same transformation on test data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1a2d3-9485-4cef-984d-43fc688a0392",
   "metadata": {},
   "source": [
    "### Feature Scaling - Normalization | MinMaxScaling | MaxAbsScaling | RobustScaling\n",
    "\n",
    "**Normalization** involves rescaling the features of your dataset to a specific range, typically [0, 1] or [-1, 1], depending on the scaling technique. This is essential for machine learning models that are sensitive to the magnitude of input features.\n",
    "\n",
    "**Why Use It?**  \n",
    "Many machine learning algorithms like **logistic regression**, **support vector machines (SVM)**, and **neural networks** can perform better when features are on the same scale. Different scaling techniques suit different types of data and tasks.\n",
    "\n",
    "**Types of Scaling**:\n",
    "- **MinMaxScaling**: Scales features to a given range, usually [0, 1]. Best suited for algorithms that assume the features are bounded within a specific range.\n",
    "- **MaxAbsScaling**: Scales features by their maximum absolute value, leaving the sign unchanged. Ideal for data that is already centered at zero but needs rescaling.\n",
    "- **RobustScaling**: Uses the median and interquartile range (IQR) for scaling, making it more robust to outliers compared to MinMaxScaling.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- **Fit** the scaler on the **training data only** to avoid data leakage.\n",
    "- **Transform** both training and test datasets with the same scaler to ensure consistency.\n",
    "- MinMaxScaling can be sensitive to outliers, while RobustScaling handles them better.\n",
    "  \n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "min_max_scaler = MinMaxScaler()\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit on the training data only and transform both train and test data\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9657e6-e69f-42ad-a74a-816625bba7b9",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data | Ordinal Encoding | Label Encoding\n",
    "\n",
    "**Categorical Encoding** is essential when dealing with features that contain categorical data, as most machine learning algorithms expect numerical input. Two common types of encoding are **Ordinal Encoding** and **Label Encoding**.\n",
    "\n",
    "**Why Use It?**\n",
    "Machine learning algorithms cannot handle raw categorical data, and encoding is necessary to convert these categories into numerical values that models can understand.\n",
    "\n",
    "#### Types of Encoding:\n",
    "- **Ordinal Encoding**: This encoding is used when the categorical variable has an inherent order (e.g., \"low\", \"medium\", \"high\"). Each category is assigned an integer based on the rank.\n",
    "- **Label Encoding**: Assigns a unique integer to each category, with no inherent order. This is useful for nominal data, where no ordering exists.\n",
    "\n",
    "**Use Cases**:\n",
    "- **Ordinal Encoding** is suitable for ordinal features with meaningful rankings.\n",
    "- **Label Encoding** is generally used for nominal features (without order) but can introduce issues with algorithms that assume some ordering from integer values.\n",
    "\n",
    "**Column Transformer**: This is used when you need to apply different preprocessing techniques (like encoding or scaling) to different columns of your dataset. It is especially useful for pipelines with both categorical and numerical features.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Ordinal encoding should only be applied when the order of categories matters.\n",
    "- Label encoding can mislead some models if there is no inherent ordering but numbers are assigned. Consider **One-Hot Encoding** for such cases.\n",
    "- Fit the encoder on the training data and transform both train and test datasets.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "X = [['low'], ['medium'], ['high']]\n",
    "\n",
    "# Ordinal Encoding (for ordinal features)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "X_ordinal_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# Label Encoding (for nominal features)\n",
    "label_encoder = LabelEncoder()\n",
    "y = ['cat', 'dog', 'mouse']\n",
    "y_label_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Column Transformer - Example (if needed)\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(), ['categorical_column']),\n",
    "    ('num', 'passthrough', ['numerical_column'])\n",
    "])\n",
    "\n",
    "# Apply the transformations to the dataset\n",
    "X_transformed = column_transformer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42139ed6-d455-4d88-a308-1763bfd83b60",
   "metadata": {},
   "source": [
    "### One Hot Encoding | Handling Categorical Data\n",
    "\n",
    "**One-Hot Encoding** is a technique used to convert categorical data into a binary (0 or 1) matrix format. Each unique category is transformed into a separate column, where the presence of a category is marked as 1 and the absence as 0.\n",
    "\n",
    "**Why Use It?**  \n",
    "Unlike **Label Encoding**, which can introduce a false ordinal relationship between categories, One-Hot Encoding ensures that no such ordering is implied. It's particularly useful for nominal categorical data where categories have no order.\n",
    "\n",
    "#### Multicollinearity Concern:\n",
    "- **Multicollinearity** can arise when the columns created by one-hot encoding are not independent. To avoid this, we drop one column from the set of n categories (thus creating n-1 columns). This helps avoid the **Dummy Variable Trap**, where the presence of a category is perfectly predicted by the other categories.\n",
    "\n",
    "**Key Points to Keep in Mind**:\n",
    "- Use **One-Hot Encoding** when categories are **nominal** and there's no natural order.\n",
    "- Always ensure input columns are **independent** after encoding.\n",
    "- For categorical features with n categories, One-Hot Encoding will create **n-1 columns** to avoid multicollinearity.\n",
    "- Avoid the dummy variable trap by dropping one category from the encoded matrix.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi']\n",
    "})\n",
    "\n",
    "# One-Hot Encoding using pandas\n",
    "# We use drop_first=True to avoid the Dummy Variable Trap (n-1 columns)\n",
    "df_encoded = pd.get_dummies(df, columns=['City'], drop_first=True)\n",
    "\n",
    "# Example output would be:\n",
    "#    City_Lahore  City_Islamabad  City_Karachi\n",
    "# 0            0               0             1\n",
    "# 1            1               0             0\n",
    "# 2            0               1             0\n",
    "# 3            0               0             1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a32bb-e303-42a0-8973-e51f45d40cb4",
   "metadata": {},
   "source": [
    "### Column Transformer in Machine Learning\n",
    "\n",
    "The **Column Transformer** is a useful tool in machine learning when you need to apply different preprocessing techniques to different types of features (columns). For instance, you might want to apply **scaling** to numerical data while applying **encoding** to categorical data simultaneously.\n",
    "\n",
    "**Why Use It?**  \n",
    "It streamlines the preprocessing of datasets that contain both numerical and categorical variables, enabling you to handle each column type appropriately without manually applying transformations to each column.\n",
    "\n",
    "#### Key Use Cases:\n",
    "- **Numerical columns**: You may want to scale or normalize these features.\n",
    "- **Categorical columns**: You can apply encoders such as **One-Hot Encoding** or **Ordinal Encoding**.\n",
    "- **Pipelines**: The Column Transformer works well in conjunction with machine learning pipelines for streamlined workflows.\n",
    "\n",
    "#### Important Points to Consider:\n",
    "- Ensure the right transformation is applied to the correct column type.\n",
    "- You can chain transformers together to process different columns efficiently.\n",
    "- It's essential to fit the column transformer on training data and then apply it to both training and test sets to avoid **data leakage**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 32, 40, 50],\n",
    "    'city': ['Karachi', 'Lahore', 'Islamabad', 'Lahore'],\n",
    "    'salary': [50000, 60000, 65000, 70000]\n",
    "})\n",
    "\n",
    "# Column Transformer setup\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), ['age', 'salary']),  # Scaling numerical data\n",
    "    ('cat', OneHotEncoder(drop='first'), ['city'])  # One-Hot Encoding categorical data\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = column_transformer.fit_transform(df)\n",
    "\n",
    "# The numerical columns are scaled, and the categorical column is one-hot encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77278-f3ec-4544-9e86-71bf03cd87ed",
   "metadata": {},
   "source": [
    "### Function Transformer | Log Transform | Reciprocal Transform | Square Root Transform\n",
    "\n",
    "**Function Transformers** allow you to apply mathematical transformations to your features. Some of the common transformations include:\n",
    "- **Log Transform**: Helps handle skewed data by reducing the effect of large outliers. Used when data is highly right-skewed.\n",
    "- **Reciprocal Transform**: Similar to log transform but stronger. Works well with extreme values.\n",
    "- **Square Root Transform**: Less aggressive than log, used for moderately skewed data.\n",
    "\n",
    "These transformations are typically used when you want to make non-normally distributed data more Gaussian (normal), which helps improve model performance for algorithms that assume normality (e.g., linear regression, SVM).\n",
    "\n",
    "#### How to Check if Distribution is Normal?\n",
    "\n",
    "1. **sns.distplot()**: This plots the distribution of your data.\n",
    "   - If the plot shows a bell-shaped curve, the data is normally distributed.\n",
    "   \n",
    "2. **pd.skew()**: Returns the skewness of the data. \n",
    "   - If the skew is close to 0, the data is symmetric. \n",
    "   - Positive values indicate right-skewed data, and negative values indicate left-skewed data.\n",
    "   \n",
    "3. **QQ Plot**: (Quantile-Quantile plot) checks the normality by comparing the quantiles of your data against a theoretical normal distribution.\n",
    "   - If the points lie on the diagonal line, the data is normally distributed.\n",
    "\n",
    "#### Key Points:\n",
    "- **Log Transform**: Use for right-skewed data.\n",
    "- **Reciprocal Transform**: Use for data with extreme values.\n",
    "- **Square Root Transform**: Use for moderately skewed data.\n",
    "- Checking the data's distribution is crucial before applying transformations.\n",
    "- After transforming, always recheck the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'income': [1000, 3000, 10000, 50000, 100000, 200000]\n",
    "})\n",
    "\n",
    "# Checking Distribution with Seaborn's distplot\n",
    "sns.distplot(df['income'])\n",
    "plt.show()\n",
    "\n",
    "# Checking Skewness\n",
    "print(f\"Skewness: {df['income'].skew()}\")\n",
    "\n",
    "# QQ Plot to check normality\n",
    "stats.probplot(df['income'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# Applying Log Transform to reduce skewness\n",
    "df['income_log'] = np.log(df['income'] + 1)\n",
    "\n",
    "# Checking the transformed distribution\n",
    "sns.distplot(df['income_log'])\n",
    "plt.show()\n",
    "\n",
    "# Checking skewness after transformation\n",
    "print(f\"Skewness after log transform: {df['income_log'].skew()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a899ab5-2953-4e64-8203-0d80505e8d39",
   "metadata": {},
   "source": [
    "### Power Transformer | Box-Cox Transform | Yeo-Johnson Transform\n",
    "\n",
    "**Power Transformations** are used to make data more Gaussian-like by stabilizing variance and minimizing skewness. They are particularly useful when dealing with **heteroscedasticity** or non-normal distributions in features.\n",
    "\n",
    "#### **Box-Cox Transform**:\n",
    "- Requires input data to be **strictly positive**.\n",
    "- A parameter, **lambda**, controls the transformation (range from -5 to 5).\n",
    "- It works by finding an optimal lambda value that minimizes skewness.\n",
    "  \n",
    "#### **Yeo-Johnson Transform**:\n",
    "- Similar to Box-Cox but works with **both positive and negative** values.\n",
    "- More flexible than Box-Cox, making it suitable for a wider range of data distributions.\n",
    "  \n",
    "#### **Power Transformer**:\n",
    "- A generalized technique that includes Box-Cox and Yeo-Johnson transformations.\n",
    "- Automatically chooses the most appropriate transformation and finds the best lambda to make data more Gaussian.\n",
    "\n",
    "#### Key Points:\n",
    "- **Yeo-Johnson** is better because it handles negative values and zeroes.\n",
    "- **Box-Cox** is restricted to positive data only.\n",
    "- **Power Transformer** can handle different data ranges and provides more flexibility with both **Yeo-Johnson** and **Box-Cox** under its umbrella.\n",
    "- After transforming the data, always verify whether the transformation was successful using skewness, distplot, or QQ plots.\n",
    "\n",
    "#### When to Use:\n",
    "- Use these transformations when data is heavily skewed and you need to normalize it for better model performance, particularly for algorithms like linear regression, which assume normality.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with negative and positive values\n",
    "df = pd.DataFrame({'income': [-1000, 2000, 5000, 10000, 15000, 50000, 100000]})\n",
    "\n",
    "# Visualizing original distribution\n",
    "sns.distplot(df['income'])\n",
    "plt.title(\"Original Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Applying Power Transformer (Yeo-Johnson by default)\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "df['income_transformed'] = power_transformer.fit_transform(df[['income']])\n",
    "\n",
    "# Visualizing transformed distribution\n",
    "sns.distplot(df['income_transformed'])\n",
    "plt.title(\"Transformed Distribution (Yeo-Johnson)\")\n",
    "plt.show()\n",
    "\n",
    "# Checking the lambda value (for reference)\n",
    "print(f\"Lambda used for transformation: {power_transformer.lambdas_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1491c4-2bcf-4c8c-935f-4b425a8dfb25",
   "metadata": {},
   "source": [
    "### Binning and Binarization | Discretization | Quantile Binning | KMeans Binning\n",
    "\n",
    "**Binning** and **Binarization** are techniques to discretize continuous features into discrete categories or binary values, helping with:\n",
    "- Simplifying data interpretation\n",
    "- Handling outliers\n",
    "- Making data more robust for certain algorithms\n",
    "\n",
    "#### Types of Binning:\n",
    "\n",
    "1. **Equal Width Binning**:\n",
    "   - Divides the data into bins of equal width. Each bin covers the same range of values.\n",
    "   - Use when data is uniformly distributed.\n",
    "\n",
    "2. **Equal Frequency Binning**:\n",
    "   - Divides the data so that each bin contains approximately the same number of observations.\n",
    "   - Useful when the distribution is not uniform.\n",
    "\n",
    "3. **KMeans Binning**:\n",
    "   - Uses clustering (KMeans) to create bins based on natural groupings in the data.\n",
    "\n",
    "4. **Quantile Binning**:\n",
    "   - Divides data based on quantiles (percentiles). Each bin has approximately equal numbers of points, making it similar to equal-frequency binning but more statistically precise.\n",
    "\n",
    "#### Binarization:\n",
    "- Converts data into binary (0 or 1) values.\n",
    "- Often used for threshold-based classification or when converting continuous variables into categorical variables.\n",
    "\n",
    "#### Key Points:\n",
    "- **Equal Width** is straightforward but doesn't handle skewed data well.\n",
    "- **Equal Frequency** helps when data is unevenly distributed.\n",
    "- **KMeans Binning** identifies natural groupings in data and is more data-driven.\n",
    "- **Quantile Binning** is useful for creating balanced bins across different ranges of data.\n",
    "- **Binarization** is used when you need to convert continuous features into a binary form.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer, Binarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample data\n",
    "data = np.array([22, 23, 45, 54, 65, 76, 88, 94, 100]).reshape(-1, 1)\n",
    "\n",
    "# Equal Width Binning (bins of equal size)\n",
    "equal_width_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "binned_data_width = equal_width_binning.fit_transform(data)\n",
    "\n",
    "# Equal Frequency Binning (each bin has equal number of points)\n",
    "equal_freq_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "binned_data_freq = equal_freq_binning.fit_transform(data)\n",
    "\n",
    "# KMeans Binning\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "data_kmeans_binned = kmeans.fit_predict(data)\n",
    "\n",
    "# Quantile Binning (similar to Equal Frequency but based on quantiles)\n",
    "quantile_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "binned_data_quantile = quantile_binning.fit_transform(data)\n",
    "\n",
    "# Binarization (Converting data to binary based on threshold)\n",
    "binarizer = Binarizer(threshold=50)\n",
    "binarized_data = binarizer.fit_transform(data)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Equal Width Binning:\\n{binned_data_width}\")\n",
    "print(f\"Equal Frequency Binning:\\n{binned_data_freq}\")\n",
    "print(f\"KMeans Binning:\\n{data_kmeans_binned}\")\n",
    "print(f\"Quantile Binning:\\n{binned_data_quantile}\")\n",
    "print(f\"Binarized Data:\\n{binarized_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bce5b9-8e9d-4762-a010-60df9681b759",
   "metadata": {},
   "source": [
    "### Handling Mixed Variables | Feature Engineering\n",
    "\n",
    "**Mixed Variables** refer to datasets that contain both **numerical** and **categorical** data types. Effectively handling mixed variables is crucial for building robust machine learning models. The steps to deal with mixed variables include:\n",
    "\n",
    "1. **Identify Variable Types**:\n",
    "   - Understand the types of variables present in your dataset. Numerical variables can be continuous (e.g., income) or discrete (e.g., number of children), while categorical variables can be nominal (e.g., city names) or ordinal (e.g., satisfaction ratings).\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **Transform Categorical Variables**: Convert categorical variables into numerical formats using methods like One-Hot Encoding, Label Encoding, or Ordinal Encoding.\n",
    "   - **Scale Numerical Variables**: Apply feature scaling techniques like Standardization or Normalization to ensure numerical variables are on a similar scale.\n",
    "\n",
    "3. **Combine Features**:\n",
    "   - Create new features by combining existing numerical and categorical features. For instance, you could multiply a numerical feature by a categorical one (after encoding) to create an interaction feature.\n",
    "\n",
    "4. **Impute Missing Values**:\n",
    "   - Handle missing values differently based on variable type. For numerical variables, use methods like mean or median imputation; for categorical variables, consider the most frequent category or a new category to indicate missingness.\n",
    "\n",
    "5. **Outlier Handling**:\n",
    "   - Identify and address outliers in numerical features without affecting the categorical features.\n",
    "\n",
    "#### Important Considerations:\n",
    "- Ensure that transformations maintain the integrity of the data.\n",
    "- Be cautious about introducing multicollinearity when combining features.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Sample dataset with mixed variables\n",
    "data = {\n",
    "    'age': [22, 25, 27, 35, 45],\n",
    "    'city': ['Karachi', 'Lahore', 'Karachi', 'Lahore', 'Islamabad'],\n",
    "    'income': [50000, 60000, 80000, 75000, 90000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: One-Hot Encoding for Categorical Variables\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_city = encoder.fit_transform(df[['city']])\n",
    "encoded_df = pd.DataFrame(encoded_city, columns=encoder.get_feature_names_out(['city']))\n",
    "\n",
    "# Step 2: Scale Numerical Variables\n",
    "scaler = StandardScaler()\n",
    "scaled_income = scaler.fit_transform(df[['income']])\n",
    "scaled_income_df = pd.DataFrame(scaled_income, columns=['scaled_income'])\n",
    "\n",
    "# Combine encoded categorical and scaled numerical features\n",
    "final_df = pd.concat([df[['age']], encoded_df, scaled_income_df], axis=1)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889e67b-fdec-4c5f-9a3e-461a051e3e47",
   "metadata": {},
   "source": [
    "### Handling Missing Data | Part 1\n",
    "\n",
    "Handling missing data is an essential step in the data preprocessing phase. Missing data can arise due to various reasons, and it is crucial to handle it effectively to avoid biased results or errors during model training. There are several methods to deal with missing data, each suited for different situations.\n",
    "\n",
    "#### Techniques to Handle Missing Data:\n",
    "\n",
    "1. **Remove the Column**:\n",
    "   - If a feature (column) has a significant amount of missing data (e.g., > 60%), it may be better to drop that column, as it may not provide useful information for the model.\n",
    "   - **When to use**: If the column is not important for the analysis and the missing data is extensive, dropping the column can be an efficient solution.\n",
    "\n",
    "2. **Complete Case Analysis (CCA)**:\n",
    "   - CCA involves removing rows with any missing values in the dataset. This method assumes that the data is missing completely at random (MCAR), meaning the missingness is not related to any other feature or the outcome.\n",
    "   - **When to use**: If the missing data is sparse and missing completely at random, this method can be applied without significantly affecting the overall dataset.\n",
    "\n",
    "3. **Missing Completely at Random (MCAR)**:\n",
    "   - Data is considered MCAR when the missingness of data points does not depend on any observed or unobserved data.\n",
    "   - **Use case**: When MCAR is confirmed, CCA can be a simple and effective approach without introducing bias into the analysis.\n",
    "\n",
    "#### Considerations for CCA:\n",
    "- **When to apply CCA**:\n",
    "   - CCA is generally recommended when the proportion of missing data is small, and the missing data does not introduce bias into the dataset.\n",
    "   - If too many rows are removed, it may lead to a loss of valuable information, so use with caution.\n",
    "   \n",
    "---\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Age': [25, 30, None, 22, 35],\n",
    "    'Income': [50000, None, 60000, 65000, 70000],\n",
    "    'City': ['Karachi', None, 'Lahore', 'Islamabad', 'Lahore']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove column if too many missing values (example: if more than 60% of values are missing)\n",
    "if df['Income'].isnull().mean() > 0.6:\n",
    "    df = df.drop(columns=['Income'])\n",
    "\n",
    "# Step 2: Complete Case Analysis (Remove rows with any missing values)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display cleaned DataFrame\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3d286-fcb1-4aa2-ac15-9af494d8d15f",
   "metadata": {},
   "source": [
    "### Handling Missing Data | Numerical Data | Simple Imputer\n",
    "\n",
    "When handling missing numerical data, it's important to decide on the imputation technique based on the distribution of the data. Imputation is essential because many machine learning algorithms do not accept missing values, and removing rows or columns might lead to data loss. There are two primary types of imputation techniques:\n",
    "\n",
    "#### Types of Imputation:\n",
    "\n",
    "1. **Univariate Imputation**:\n",
    "   - In this approach, missing values in each column are imputed independently.\n",
    "   - Common methods include imputing with the mean, median, or a constant (arbitrary value).\n",
    "\n",
    "2. **Multivariate Imputation**:\n",
    "   - This method considers the relationships between different columns while imputing missing values, which can capture correlations between variables.\n",
    "\n",
    "#### Imputation Techniques for Numerical Data:\n",
    "\n",
    "- **Mean Imputation**:\n",
    "  - This method replaces missing values with the mean of the column.\n",
    "  - **When to use**: If the distribution of the data is almost normal, this is a reasonable approach.\n",
    "\n",
    "- **Median Imputation**:\n",
    "  - Replaces missing values with the median of the column.\n",
    "  - **When to use**: If the distribution is slightly skewed, the median is a better choice as it is less affected by outliers.\n",
    "\n",
    "- **Arbitrary Value Imputation**:\n",
    "  - Missing values are replaced with an arbitrary value such as 0, -999, etc.\n",
    "  - **Use case**: When the missing data itself may carry information or when imputing with a domain-specific value is required.\n",
    "\n",
    "- **End of Distribution Imputation**:\n",
    "  - This technique fills missing values with values at the extreme end of the distribution.\n",
    "  - **Use case**: When the data is skewed, and extreme values might signify something meaningful.\n",
    "\n",
    "- **Random Sample Imputation**:\n",
    "  - Missing values are imputed by randomly selecting from the available data points of the column.\n",
    "  - **Use case**: Helps preserve the distribution of the feature.\n",
    "\n",
    "#### Choosing between Mean and Median:\n",
    "- **Mean**: Use it when the distribution is almost normal (symmetric, bell-shaped curve).\n",
    "- **Median**: Use it when the distribution is skewed (i.e., has outliers).\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing numerical values\n",
    "data = {\n",
    "    'Age': [25, 30, None, 22, 35],\n",
    "    'Income': [50000, None, 60000, 65000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Use SimpleImputer for mean imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df['Age_mean_imputed'] = mean_imputer.fit_transform(df[['Age']])\n",
    "\n",
    "# Step 2: Use SimpleImputer for median imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df['Income_median_imputed'] = median_imputer.fit_transform(df[['Income']])\n",
    "\n",
    "# Display DataFrame after imputation\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb53bd-8731-4149-89d7-3547ebb1189b",
   "metadata": {},
   "source": [
    "### Handling Missing Categorical Data | Simple Imputer | Most Frequent Imputation | Missing Category Imputation\n",
    "\n",
    "When dealing with missing categorical data, it is essential to choose an imputation strategy that preserves the distribution and meaning of the data. Unlike numerical data, where mean or median might be used, categorical data is best handled with mode or frequent category imputation.\n",
    "\n",
    "#### Common Techniques for Categorical Data Imputation:\n",
    "\n",
    "1. **Most Frequent (Mode) Imputation**:\n",
    "   - The most common value (mode) in the column is used to replace the missing values.\n",
    "   - **Use case**: If the missing data is assumed to follow the distribution of the rest of the data, this method works well.\n",
    "\n",
    "2. **Missing Category Creation**:\n",
    "   - A new category is created to explicitly indicate missing data. This is useful when missing data itself may carry some information.\n",
    "   - **Use case**: This approach is useful when you do not want to guess the missing values and prefer to label them as 'missing' or some other placeholder.\n",
    "\n",
    "#### Key Points:\n",
    "- **Most Frequent (Mode) Imputation** is widely used for categorical data because categorical values usually have a \"majority class\" that is repeated often, making it a good candidate to replace missing values.\n",
    "- **Mean/Median** is only applicable to numerical data; for categorical data, **mode** or custom categories are better suited.\n",
    "- **Custom Category Creation**: Creating a specific \"missing\" label helps avoid potential biases that could occur if an incorrect mode is imputed.\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing categorical values\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', None, 'Female', 'Male'],\n",
    "    'Occupation': [None, 'Engineer', 'Doctor', 'Teacher', None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Most Frequent (Mode) Imputation\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['Gender_mode_imputed'] = mode_imputer.fit_transform(df[['Gender']])\n",
    "\n",
    "# Step 2: Creating a new category for missing data\n",
    "missing_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "df['Occupation_missing_imputed'] = missing_imputer.fit_transform(df[['Occupation']])\n",
    "\n",
    "# Display DataFrame after imputation\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c47d5e-2f50-4baa-9e14-11cf073c1820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
