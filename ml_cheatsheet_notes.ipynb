{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f3b6a9-9c13-4c7e-8e91-2014f9847777",
   "metadata": {},
   "source": [
    "# Cheat Sheet: Key Steps and Decisions for MLP & NLP Mastery Challenge\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Initial Data Handling\n",
    "- **Load the dataset**:\n",
    "    - Use `pandas.read_csv()` or equivalent.\n",
    "    - **Check missing values**: `df.isnull().sum()`.\n",
    "\n",
    "- **Split your data**:\n",
    "    - Use `train_test_split` from `sklearn` (80/20 or 70/30 split).\n",
    "  \n",
    "- **Explore the dataset**:\n",
    "    - Use `df.describe()`, `df.info()`.\n",
    "    - Plot distributions with `sns.pairplot(df)` and `sns.heatmap(df.corr())`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Preprocessing\n",
    "- **Feature Scaling**:\n",
    "    - Use `StandardScaler` for normal distributions.\n",
    "    - Use `MinMaxScaler` or `RobustScaler` for non-normal distributions or data with outliers.\n",
    "\n",
    "- **Handle Missing Data**:\n",
    "    - **Numerical missing data**: `SimpleImputer`, `KNNImputer`.\n",
    "    - **Categorical missing data**: Use **Most Frequent Imputation** or add a new \"Missing\" category.\n",
    "\n",
    "- **Encoding Categorical Data**:\n",
    "    - Use `LabelEncoder` for binary categorical data.\n",
    "    - Use `OneHotEncoder` for multi-class categorical data.\n",
    "    - Use `ColumnTransformer` for applying transformations to specific columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Engineering\n",
    "- **Handle Date/Time Variables**:\n",
    "    - Extract day, month, year, etc.\n",
    "\n",
    "- **Feature Construction**:\n",
    "    - Create new features from existing ones (interaction terms, binning).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Outlier Detection & Removal\n",
    "- **Handle Outliers**:\n",
    "    - **Z-score** for normal data.\n",
    "    - **IQR Method** for non-normal data.\n",
    "    - **Winsorization** for capping outliers without removal.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Selection\n",
    "- **Algorithm choice**:\n",
    "    - **Classification**: Start with **Logistic Regression**, **KNN**, or **Random Forest**.\n",
    "    - **Regression**: Use **Linear Regression** or **Gradient Boosting**.\n",
    "    - **Dimensionality Reduction**: Use **PCA** if needed.\n",
    "\n",
    "- **Consider ensemble methods** like **Bagging**, **Boosting**, or **Stacking**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Training\n",
    "- **Fit the model**:\n",
    "    - `model.fit(X_train, y_train)`.\n",
    "\n",
    "- **Check for overfitting**:\n",
    "    - Use **cross-validation** (`cross_val_score`).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Evaluation\n",
    "- **For classification**:\n",
    "    - Evaluate with **accuracy**, **precision**, **recall**, **F1-score**, **confusion matrix**.\n",
    "    - For **imbalanced data**, use **F1-score** or **AUC-ROC curve**.\n",
    "\n",
    "- **For regression**:\n",
    "    - Evaluate using **MSE**, **RMSE**, **R2 Score**.\n",
    "  \n",
    "- **Plot performance**:\n",
    "    - Confusion matrices, ROC curves, and residual plots.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Hyperparameter Tuning\n",
    "- **Optimize parameters** using:\n",
    "    - **GridSearchCV** or **RandomizedSearchCV**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Post-Model Analysis\n",
    "- **Check feature importance**:\n",
    "    - Use `model.feature_importances_` for tree-based models like **Random Forest**.\n",
    "\n",
    "- **Evaluate model generalization**:\n",
    "    - Analyze test set performance and cross-validation scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Model Interpretability\n",
    "- **Interpret the model**:\n",
    "    - Use **SHAP** or **LIME** for interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Model Comparison\n",
    "- **Compare multiple models**:\n",
    "    - Train several models and compare their performance.\n",
    "    - Visualize model performance using bar charts for accuracy or relevant metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Final Notes\n",
    "- **Avoid common pitfalls**:\n",
    "    - Donâ€™t forget to **scale your data** (especially for distance-based algorithms).\n",
    "    - Always split your data into **train/test sets** to avoid data leakage.\n",
    "    - **Document your code** for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Algorithms to Remember:\n",
    "- **Logistic Regression** for binary classification.\n",
    "- **KNN** for non-parametric tasks.\n",
    "- **SVM** for complex decision boundaries.\n",
    "- **Random Forest** for feature importance.\n",
    "- **Gradient Boosting** for performance optimization.\n",
    "- **K-Means** for clustering.\n",
    "- **PCA** for dimensionality reduction.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
