{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3523f6ac-9f74-45ba-9469-b5a2677e1f64",
   "metadata": {},
   "source": [
    "# Summary of Machine Learning Workflow\n",
    "\n",
    "This notebook outlines a step-by-step guide to performing an end-to-end machine learning workflow, applicable to various datasets. Below is a breakdown of each step and its purpose:\n",
    "\n",
    "1. **Load and Understand Your Data**: This is the initial step where you load the dataset and inspect its structure, types of features (numerical/categorical), and any missing values. This helps in understanding the dataset's overall characteristics before proceeding with analysis.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: EDA helps uncover relationships, distributions, and patterns in the data using visualizations like histograms, bar charts, and correlation matrices. It helps you understand the data's underlying trends and guides you toward relevant preprocessing steps.\n",
    "\n",
    "3. **Handling Missing Data**: Missing data can skew the model. Depending on the feature type, we impute missing values using strategies like mean or most frequent value. You can also drop features if missing data is too significant.\n",
    "\n",
    "4. **Handling Outliers**: Outliers can distort models. By using methods like Z-scores or IQR, we can detect and potentially remove these extreme values, which might help improve model performance.\n",
    "\n",
    "5. **Feature Engineering**: This involves transforming raw data into meaningful features. For categorical data, you can apply label or one-hot encoding. You can also extract useful information from date/time features or create new features from existing ones to improve model predictions.\n",
    "\n",
    "6. **Feature Scaling**: Scaling ensures numerical features have comparable ranges, which is important for algorithms that rely on distances (like SVM or KNN). Techniques like standardization or min-max scaling are used to bring features to a consistent range.\n",
    "\n",
    "7. **Train-Test Split**: We split the data into training and testing sets to ensure the model can generalize well on unseen data. A typical split is 80% for training and 20% for testing.\n",
    "\n",
    "8. **Model Building**: Depending on the task, we choose a classification model (like Random Forest for categorical target variables) or a regression model (like Linear Regression for continuous target variables). We fit the model to the training data.\n",
    "\n",
    "9. **Model Evaluation**: For classification tasks, we evaluate performance using accuracy, confusion matrix, and classification report. For regression, metrics like mean squared error (MSE) and R2 score are used. These metrics help in understanding the model's prediction quality.\n",
    "\n",
    "10. **Handling Imbalanced Data**: If the target classes are imbalanced (e.g., one class occurs much more frequently), techniques like SMOTE can be used to balance the dataset, improving model performance on minority classes.\n",
    "\n",
    "11. **Hyperparameter Tuning**: Finally, we fine-tune the model's hyperparameters using techniques like Grid Search to optimize performance. This helps find the best combination of settings for the model to perform optimally on the data.\n",
    "\n",
    "In summary, this workflow ensures that you follow a logical, ordered approach to prepare, model, and evaluate data in any machine learning project. Each step is designed to address specific issues in the dataset, making your model robust and accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c4d08-bd7f-437a-b9e2-55392b1dd2f9",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow for Any Dataset\n",
    "\n",
    "This notebook covers an end-to-end workflow for machine learning projects, providing step-by-step guidance on how to work with a new dataset. Each section is described with the potential variations and solutions you may face in different datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load and Understand Your Data\n",
    "\n",
    "Before you do any processing, load the dataset and explore its basic structure.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace with your dataset file)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Check the first few rows to understand the structure\n",
    "print(df.head())\n",
    "\n",
    "# Summary of the dataframe\n",
    "print(df.info())\n",
    "\n",
    "# Descriptive statistics for numerical columns\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadf76a-e1a9-4fc4-a6da-640f046fccca",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. The dataset is too large? -> Use `.sample()` instead of `.head()` to see random samples.\n",
    "2. Missing values are pervasive? -> Handle missing values based on the feature type (categorical or numerical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65589db1-187b-4e8c-b739-24d5149ca5df",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Explore relationships and distributions within your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89f2f0-5147-47d8-81f5-fc7f999c1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting histograms for all numerical features\n",
    "df.hist(figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix to see relationships between numerical features\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Unique values in categorical columns (for understanding categorical data)\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(f'{column}: {df[column].nunique()} unique values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44801b8-e9de-403e-bbf2-64c7b3d4203a",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. You have a lot of categorical data? -> Use bar plots to visualize the distribution of each category.\n",
    "2. You don't find meaningful correlations? -> Consider feature engineering to construct new features or discard weak ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c632254-3502-4f77-b47b-4f83b3386490",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Data\n",
    "\n",
    "Missing data can impact your model. Here are ways to handle them depending on the data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee39cb-1145-40a8-ad19-960ea6c1d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Filling missing values with the mean\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df['numerical_column'] = num_imputer.fit_transform(df[['numerical_column']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5584d1-5cac-40a9-88c9-23da3f9d8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the most frequent category\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['categorical_column'] = cat_imputer.fit_transform(df[['categorical_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13ece5-17be-4d15-bce1-06e7f6b6af6b",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. There are too many missing values? -> You can drop columns with excessive missing data.\n",
    "2. Missing values are in categorical data? -> Use 'missing' as a category or most frequent imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815a3e3-5572-4edc-808c-19bd77cc8562",
   "metadata": {},
   "source": [
    "## 4. Handling Outliers\n",
    "\n",
    "Outliers can distort your model. Detect and handle them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562f80e-2154-4d21-a690-42dbd36fb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Z-score method to detect outliers in a numerical column\n",
    "z_scores = np.abs(stats.zscore(df['numerical_column']))\n",
    "df_cleaned = df[(z_scores < 3)]  # Filter out rows with Z-scores > 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16487a1c-8949-4192-9d69-c2d95c3d0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Removing outliers\n",
    "df_cleaned = df[~((df['numerical_column'] < (Q1 - 1.5 * IQR)) | \n",
    "                  (df['numerical_column'] > (Q3 + 1.5 * IQR)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136585d-310b-4c0b-90b4-d0244136cf2e",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. Your model is not sensitive to outliers? -> You can choose to keep them.\n",
    "2. Too many outliers exist? -> Try using robust methods like robust scaling or Winsorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46585381-cae4-461d-bc13-d0f749753fa4",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fe667-4005-45e8-9ac4-3592c380b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Label Encoding for ordinal data\n",
    "le = LabelEncoder()\n",
    "df['ordinal_column'] = le.fit_transform(df['ordinal_column'])\n",
    "\n",
    "# One-Hot Encoding for nominal data\n",
    "df = pd.get_dummies(df, columns=['nominal_column'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc4bca-473b-45e3-a7f1-4a15b496afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day from datetime columns\n",
    "df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "df['year'] = df['date_column'].dt.year\n",
    "df['month'] = df['date_column'].dt.month\n",
    "df['day'] = df['date_column'].dt.day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966b5b3-f0c9-42fd-a017-41c00f375f92",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. There are too many categories in a feature? -> Use target encoding or combine similar categories.\n",
    "2. Date data isn't useful? -> You can drop it, but be cautious of losing time-sensitive patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc1d42-f47a-4b4a-aed7-13ad84fbc326",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling\n",
    "\n",
    "Scale the numerical features so that they contribute equally to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef02f2-2b70-432b-ae25-396eb26c10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "df[['num_col1', 'num_col2']] = scaler.fit_transform(df[['num_col1', 'num_col2']])\n",
    "\n",
    "# Min-Max Scaling (between 0 and 1)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[['num_col1', 'num_col2']] = min_max_scaler.fit_transform(df[['num_col1', 'num_col2']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e90f1-2363-4f8b-825b-7408a9c1d5ba",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split\n",
    "\n",
    "Separate your data into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764e1ce-faed-4d43-9a5e-5d181ea5fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target_column', axis=1)  # Features\n",
    "y = df['target_column']  # Target\n",
    "\n",
    "# Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527987a-b68b-4c19-a8d8-d6ba07c7d077",
   "metadata": {},
   "source": [
    "## 8. Model Building\n",
    "\n",
    "Choose a model depending on your task (classification or regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf33b7-53cf-4ce8-a269-9b6338605ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and fit the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619a7e7-f618-4172-93ca-d54fb8625e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize and fit the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75156b8-0213-4a1c-ae49-c687f039ad7b",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c93cbb-5986-4365-a6d6-df381b04c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate classification performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c350d4-5c6b-4113-920e-d0f6f98fad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate regression performance\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2151a-bf43-452f-8447-9350f06ee420",
   "metadata": {},
   "source": [
    "## 10. Handling Imbalanced Data\n",
    "\n",
    "If your dataset is imbalanced (classification), use techniques like SMOTE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df956a67-bdb2-4f74-a9d7-08094122cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f6625-cb1a-430d-b970-36fc9c91a401",
   "metadata": {},
   "source": [
    "### What if:\n",
    "1. SMOTE doesnâ€™t work well? -> Try other techniques like undersampling or adjusting class weights in the model.\n",
    "2. The data is not imbalanced? -> Skip this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825ff0b-a8c5-4d17-b30a-7a9d920f1ca4",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning\n",
    "\n",
    "Fine-tune your model to improve performance using Grid Search or Randomized Search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5e2c3-ed01-4da8-b57f-a8b3bb82b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example for tuning RandomForest hyperparameters\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
