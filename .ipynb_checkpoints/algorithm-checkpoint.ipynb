{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1ce7b9-9bf8-49e0-b287-1d476e38bc8e",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "1. **Advantages**:\n",
    "   - Simple and easy to interpret.\n",
    "   - Works well for binary classification problems.\n",
    "   - Fast to train and can handle large datasets efficiently.\n",
    "   - Provides probabilistic predictions (outputs probabilities).\n",
    "   \n",
    "2. **Disadvantages**:\n",
    "   - Assumes linear relationship between features and log-odds.\n",
    "   - Sensitive to multicollinearity.\n",
    "   - May underperform on more complex datasets.\n",
    "\n",
    "3. **Use Case**: Predicting whether an email is spam or not (binary classification).\n",
    "\n",
    "4. **Consider**: Features should not be highly correlated. Consider regularization if overfitting occurs.\n",
    "\n",
    "5. **Best for**: Small to medium-sized datasets with linearly separable classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24636cf-9724-4bb8-839d-c5f7505fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example usage\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525a2c5-71cd-4ded-81f6-688d519ddc33",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "1. **Advantages**:\n",
    "   - Easy to interpret and visualize.\n",
    "   - Can handle both numerical and categorical data.\n",
    "   - No need for feature scaling or normalization.\n",
    "   - Works well on both regression and classification problems.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Prone to overfitting, especially on small datasets.\n",
    "   - Sensitive to small changes in the data (high variance).\n",
    "   - Does not generalize well compared to ensemble methods.\n",
    "\n",
    "3. **Use Case**: Diagnosing diseases based on symptoms (classification).\n",
    "\n",
    "4. **Consider**: Use pruning techniques or max depth to avoid overfitting.\n",
    "\n",
    "5. **Best for**: Datasets with complex, non-linear relationships between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa087dc-3585-4bae-852e-5ae72e606740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Example usage\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab68af-01ec-4c2a-9ab9-12cf281e0a58",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "1. **Advantages**:\n",
    "   - Reduces overfitting compared to decision trees.\n",
    "   - Can handle missing values and maintain accuracy.\n",
    "   - Performs well on large datasets and non-linear data.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Slower to train than a single decision tree.\n",
    "   - Difficult to interpret compared to a single decision tree.\n",
    "   - Can be memory-intensive on large datasets.\n",
    "\n",
    "3. **Use Case**: Predicting customer churn (classification and regression).\n",
    "\n",
    "4. **Consider**: For large datasets, adjust the number of trees (n_estimators) to balance performance and speed.\n",
    "\n",
    "5. **Best for**: Large datasets with high-dimensional and complex features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281902d-f6e1-4076-8788-51e146e9a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Example usage\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962065fc-e939-42c8-b1a0-9771a3e4981c",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN)\n",
    "1. **Advantages**:\n",
    "   - Simple and easy to understand.\n",
    "   - No assumptions about data distribution.\n",
    "   - Works well for small datasets.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Sensitive to irrelevant or redundant features.\n",
    "   - Slow prediction time for large datasets as it needs to compare with every data point.\n",
    "   - Requires feature scaling for best performance.\n",
    "\n",
    "3. **Use Case**: Recommender systems, image classification.\n",
    "\n",
    "4. **Consider**: Use k-fold cross-validation to determine the best value for k.\n",
    "\n",
    "5. **Best for**: Small datasets with low noise and numerical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f8a5f-549f-4124-9f4d-e7c272f31ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Example usage\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3ec07-1b32-44ae-87da-36f80151d133",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "1. **Advantages**:\n",
    "   - Effective in high-dimensional spaces.\n",
    "   - Works well with a clear margin of separation.\n",
    "   - Memory efficient due to support vectors.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Slow to train on large datasets.\n",
    "   - Does not work well with overlapping classes.\n",
    "   - Requires careful tuning of hyperparameters.\n",
    "\n",
    "3. **Use Case**: Handwritten digit recognition (classification).\n",
    "\n",
    "4. **Consider**: Use kernel functions (linear, polynomial, RBF) depending on the complexity of the data.\n",
    "\n",
    "5. **Best for**: High-dimensional data with a clear margin between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507d0a5-d28e-4ac4-a0e1-eb271abf60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Example usage\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4417f5-3c33-41e4-959b-a577df5d79c1",
   "metadata": {},
   "source": [
    "# k-Means Clustering\n",
    "1. **Advantages**:\n",
    "   - Simple and easy to implement.\n",
    "   - Efficient for large datasets.\n",
    "   - Works well when clusters are well-separated.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Requires pre-specifying the number of clusters (k).\n",
    "   - Sensitive to outliers and noise.\n",
    "   - Assumes spherical clusters, which may not fit real-world data well.\n",
    "\n",
    "3. **Use Case**: Customer segmentation in marketing (clustering).\n",
    "\n",
    "4. **Consider**: Use the elbow method to find the optimal value for k.\n",
    "\n",
    "5. **Best for**: Large datasets with clear and distinct clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236de95a-475f-430d-be67-53e8a3114daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Example usage\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X_train)\n",
    "clusters = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f842c39-31cf-4482-8759-5802feff1eb0",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "1. **Advantages**:\n",
    "   - Reduces dimensionality, improving model performance and visualization.\n",
    "   - Removes multicollinearity by transforming features into principal components.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Loses interpretability of the original features.\n",
    "   - Sensitive to outliers.\n",
    "\n",
    "3. **Use Case**: Dimensionality reduction for image recognition tasks.\n",
    "\n",
    "4. **Consider**: Standardize data before applying PCA to ensure optimal performance.\n",
    "\n",
    "5. **Best for**: High-dimensional datasets where feature reduction is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e471fb5-7a1e-4696-b1d1-2458921240a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example usage\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b637d-932e-4b8d-8976-985d5a315260",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "1. **Advantages**:\n",
    "   - Simple and easy to interpret.\n",
    "   - Works well for linear relationships.\n",
    "   - Fast to train and efficient on large datasets.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Assumes a linear relationship between features and output.\n",
    "   - Sensitive to outliers.\n",
    "   - May underperform on non-linear data.\n",
    "\n",
    "3. **Use Case**: Predicting house prices based on square footage (regression).\n",
    "\n",
    "4. **Consider**: Check for multicollinearity among features and scale features for best performance.\n",
    "\n",
    "5. **Best for**: Small to medium-sized datasets with a clear linear relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a4adb-b4be-464b-92a4-c55b7c922c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example usage\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ab1e8-0b97-49dd-803f-6cc909d29e53",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "1. **Advantages**:\n",
    "   - Reduces overfitting through L2 regularization.\n",
    "   - Useful when there is multicollinearity between features.\n",
    "   - Adds a penalty term to the cost function, making the model more generalizable.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - May not set coefficients to zero, thus not performing feature selection like Lasso regression.\n",
    "   - Sensitive to the choice of regularization parameter (alpha).\n",
    "\n",
    "3. **Use Case**: Predicting prices when features are highly correlated (regression).\n",
    "\n",
    "4. **Consider**: Tune the regularization strength (alpha) to find the best balance between bias and variance.\n",
    "\n",
    "5. **Best for**: Medium-sized datasets with multicollinearity among features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2456a06-6710-408c-86c6-1a1d82ab6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Example usage\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f871372-98fd-4b60-9300-18d0f8f575f0",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "1. **Advantages**:\n",
    "   - Performs feature selection by setting some coefficients to zero (L1 regularization).\n",
    "   - Reduces overfitting and increases interpretability.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - May discard important features, especially when features are highly correlated.\n",
    "   - Sensitive to the choice of regularization parameter.\n",
    "\n",
    "3. **Use Case**: Predicting real estate prices while selecting the most important features (regression).\n",
    "\n",
    "4. **Consider**: Use when you need both prediction and feature selection.\n",
    "\n",
    "5. **Best for**: Small to medium-sized datasets with many features where feature selection is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ea802-30cb-4826-baab-9bb84b761c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Example usage\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784164f5-5881-401b-a531-5c3e0cb4ef89",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "1. **Advantages**:\n",
    "   - Highly accurate and reduces both bias and variance.\n",
    "   - Works well on structured/tabular data for both regression and classification.\n",
    "   - Handles non-linear relationships well.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Slower to train compared to simpler models.\n",
    "   - Sensitive to noisy data and prone to overfitting without proper tuning.\n",
    "\n",
    "3. **Use Case**: Predicting loan defaults (classification and regression).\n",
    "\n",
    "4. **Consider**: Tune the number of trees, learning rate, and depth to prevent overfitting.\n",
    "\n",
    "5. **Best for**: Large datasets where accuracy is crucial and computational resources are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec3d60-6b98-4622-ad93-aacc70588a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Example usage\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6b545-ef5e-408a-8da9-ca51f906261a",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "1. **Advantages**:\n",
    "   - More efficient and faster than Gradient Boosting.\n",
    "   - Reduces overfitting with regularization techniques.\n",
    "   - Handles missing values well and provides feature importance.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Complex model; requires tuning to achieve optimal performance.\n",
    "   - Sensitive to noisy data.\n",
    "\n",
    "3. **Use Case**: Winning Kaggle competitions (classification and regression).\n",
    "\n",
    "4. **Consider**: Use when you have a large dataset and need a powerful, high-performing model.\n",
    "\n",
    "5. **Best for**: Large datasets with complex relationships between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ebb86-ffc2-494c-aefd-b82eb1485133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Example usage\n",
    "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c07fc2-10ea-4131-9fe5-6d3a61d3c0bc",
   "metadata": {},
   "source": [
    "# Neural Networks (MLP)\n",
    "1. **Advantages**:\n",
    "   - Can model highly complex relationships.\n",
    "   - Works well with large datasets and can capture non-linearities.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Requires large amounts of data and computation power.\n",
    "   - Difficult to interpret compared to simpler models.\n",
    "   - Prone to overfitting without proper regularization or tuning.\n",
    "\n",
    "3. **Use Case**: Predicting customer behavior in e-commerce (classification).\n",
    "\n",
    "4. **Consider**: Use when simpler algorithms fail to capture complex patterns, and you have access to computational resources.\n",
    "\n",
    "5. **Best for**: Large datasets with complex, non-linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce3972-b914-41e3-a89c-e4c51b6a9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Example usage\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c4309-d952-4d12-9c8e-bcc0259b29eb",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "1. **Advantages**:\n",
    "   - Combines weak learners (e.g., decision trees) to create a strong model.\n",
    "   - Works well with both classification and regression tasks.\n",
    "   - Reduces both bias and variance.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Can be sensitive to noisy data and outliers.\n",
    "   - Requires careful tuning of hyperparameters.\n",
    "\n",
    "3. **Use Case**: Image recognition tasks (classification).\n",
    "\n",
    "4. **Consider**: Use when you need to boost the performance of weak learners like decision trees.\n",
    "\n",
    "5. **Best for**: Medium-sized datasets where boosting can improve the performance of simple models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4936d0-c980-4843-a53c-704dd74cf7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Example usage\n",
    "model = AdaBoostClassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70efac-0e68-4398-84a6-844336df6b43",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "1. **Advantages**:\n",
    "   - Fast and easy to implement.\n",
    "   - Works well with small datasets.\n",
    "   - Effective for text classification (e.g., spam detection).\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Assumes independence between features (which may not hold in real datasets).\n",
    "   - Can perform poorly if feature independence assumption is violated.\n",
    "\n",
    "3. **Use Case**: Sentiment analysis, spam classification.\n",
    "\n",
    "4. **Consider**: Use for text classification problems with small to medium-sized datasets.\n",
    "\n",
    "5. **Best for**: Small datasets, especially text data, where features are somewhat independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7da4bb-9a58-444f-ac9b-53b20e1b09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Example usage\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09fad26-6cdc-4e09-bff1-2321809955cd",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "1. **Advantages**:\n",
    "   - Extremely fast training speed.\n",
    "   - Handles large-scale data efficiently.\n",
    "   - Supports parallel and GPU training.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Sensitive to overfitting if not properly tuned.\n",
    "   - Requires careful hyperparameter tuning.\n",
    "\n",
    "3. **Use Case**: Click-through rate (CTR) prediction, ranking tasks.\n",
    "\n",
    "4. **Consider**: Use when dealing with very large datasets, and you need fast performance.\n",
    "\n",
    "5. **Best for**: Large datasets with a high number of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fe22e-6468-4855-8143-37ea468d057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Example usage\n",
    "model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34421690-7011-4eb8-bdf8-33d40e2b3331",
   "metadata": {},
   "source": [
    "# K-Means++\n",
    "1. **Advantages**:\n",
    "   - Improvement over k-Means by selecting better initial centroids.\n",
    "   - Reduces the likelihood of poor cluster convergence.\n",
    "   - Faster than regular k-Means.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Still sensitive to outliers.\n",
    "   - Can struggle with datasets containing overlapping clusters.\n",
    "\n",
    "3. **Use Case**: Grouping customers based on purchasing behavior (clustering).\n",
    "\n",
    "4. **Consider**: Use when you want better initial cluster centers than standard k-Means.\n",
    "\n",
    "5. **Best for**: Datasets with clear cluster separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55090dd-5705-4c61-8e9c-f40b40c3991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Example usage\n",
    "model = KMeans(n_clusters=3, init='k-means++')\n",
    "model.fit(X_train)\n",
    "clusters = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc39ddd-7bac-4b59-92b3-3cb627462b31",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "1. **Advantages**:\n",
    "   - Can find arbitrarily shaped clusters.\n",
    "   - Robust to outliers.\n",
    "   - Does not require the number of clusters to be predefined.\n",
    "\n",
    "2. **Disadvantages**:\n",
    "   - Sensitive to the choice of hyperparameters (eps, min_samples).\n",
    "   - Struggles with datasets where clusters have varying densities.\n",
    "\n",
    "3. **Use Case**: Identifying clusters of similar documents in large text datasets (clustering).\n",
    "\n",
    "4. **Consider**: Use when clusters are irregularly shaped, and you don't know how many clusters there are.\n",
    "\n",
    "5. **Best for**: Medium-sized datasets with noise and irregular cluster shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25c741-0081-4bfc-bdf3-5c58c94fd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Example usage\n",
    "model = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = model.fit_predict(X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
