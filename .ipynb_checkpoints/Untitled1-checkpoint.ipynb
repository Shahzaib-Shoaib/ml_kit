{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ce0fc-5b01-4391-b711-6d0e84c13f81",
   "metadata": {},
   "source": [
    "### Understanding Your Data\n",
    "\n",
    "Before starting any machine learning project, it's essential to understand the structure and characteristics of your data. This step helps you identify key features, data types, missing values, correlations, and any potential issues such as duplicates or outliers. Understanding your data allows you to make informed decisions for preprocessing and model building.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Shape & Overview**: Helps in getting a quick idea about the size of the dataset and the types of features it contains.\n",
    "- **Missing Values**: Identifies if there are missing values that need to be handled.\n",
    "- **Descriptive Statistics**: Summarizes basic statistical properties like mean, standard deviation, and percentiles.\n",
    "- **Correlation**: Shows relationships between features, useful for feature selection and detecting multicollinearity.\n",
    "- **Duplicates**: Identifies duplicate records that might need to be removed for better data quality.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Missing Data**: Be sure to handle missing values appropriately (impute or remove) to avoid data leakage.\n",
    "- **Outliers**: Investigate whether the dataset has outliers that need to be treated.\n",
    "- **Data Types**: Ensure that your features have the correct data types (e.g., categorical, numerical) for further processing.\n",
    "- **Duplicate Data**: Always check for duplicate rows to avoid skewing your results.\n",
    "- **Correlations**: Highly correlated features can lead to multicollinearity, which may degrade model performance.\n",
    "\n",
    "---\n",
    "\n",
    "By performing these checks, you gain a solid understanding of the data and can proceed with preprocessing and feature engineering confidently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa9e74a-a009-4c8d-aa1f-c88389c37126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Understanding Your Data\n",
    "\n",
    "# Get the shape of the dataset (rows, columns)\n",
    "df.shape\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Randomly sample 5 rows from the dataset\n",
    "df.sample(5)\n",
    "\n",
    "# Get concise information about data types and non-null values\n",
    "df.info()\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "df.describe()\n",
    "\n",
    "# Check for duplicate rows\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Display correlation matrix between numerical columns\n",
    "df.corr()\n",
    "\n",
    "# Correlation of a specific column with others (e.g., 'target_column')\n",
    "df.corr()['target_column']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e65c-d6ac-4f3f-a9f2-4c581dbcf304",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of investigating datasets to discover patterns, spot anomalies, test hypotheses, and check assumptions using summary statistics and graphical representations. EDA is a crucial step in understanding the distribution and relationships between variables, preparing the data for modeling, and identifying features that may require transformation.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Data Type Identification**: Helps categorize features as numerical, categorical, or mixed.\n",
    "- **Skewness**: Understands if a distribution is skewed, which may affect model performance.\n",
    "- **Univariate & Bivariate Analysis**: Examines single-variable distributions and relationships between pairs of variables.\n",
    "- **Correlation and Interactions**: Detects multicollinearity and relationships between multiple variables using heatmaps, boxplots, and KDE (Kernel Density Estimation) plots.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Data Types (Numerical, Categorical, Mixed)**:\n",
    "   - Identifying feature types helps guide which visualizations and transformations to apply.\n",
    "   \n",
    "2. **Skewness**:\n",
    "   - Skewness checks are important for understanding the distribution shape of numerical data.\n",
    "   - Skewed data may require transformations like log or power transformations.\n",
    "\n",
    "3. **Univariate Analysis**:\n",
    "   - Looks at the distribution of a single variable. Histograms or KDE plots are used for numerical data, and bar plots for categorical data.\n",
    "\n",
    "4. **Bivariate Analysis**:\n",
    "   - Examines relationships between two variables using scatterplots, boxplots, and 2D KDE plots for numerical data.\n",
    "\n",
    "5. **2D KDE Plot**:\n",
    "   - Visualizes the bivariate distribution of two numerical variables, providing insight into their joint density.\n",
    "\n",
    "6. **Boxplot**:\n",
    "   - Useful for understanding the distribution of numerical data and identifying outliers.\n",
    "\n",
    "7. **Bar Plot**:\n",
    "   - Displays the frequency of categorical variables.\n",
    "\n",
    "8. **Heatmap**:\n",
    "   - Useful for visualizing correlations between multiple numerical features, helping detect multicollinearity.\n",
    "\n",
    "9. **Cross Tabulation (pd.crosstab)**:\n",
    "   - Compares the frequency distribution between two categorical variables.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Skewness**: Highly skewed data may require transformations.\n",
    "- **Outliers**: Boxplots help detect outliers, which may need special handling.\n",
    "- **Multicollinearity**: Heatmaps help in detecting highly correlated features, which might need to be removed.\n",
    "- **Mixed Variables**: Features with mixed data types (numerical & categorical) require special handling, such as binning for continuous data.\n",
    "\n",
    "---\n",
    "\n",
    "Performing EDA helps you better understand the data, decide on feature engineering steps, and improve the quality of inputs for your machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02524b38-b5cc-4866-986f-3bf3d04fcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for EDA\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Types (Numerical, Categorical)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Skewness\n",
    "print(df[numerical_columns].skew())\n",
    "\n",
    "# Univariate Analysis (Histogram for Numerical Columns)\n",
    "df[numerical_columns].hist(figsize=(10, 8))\n",
    "\n",
    "# Bar Plot for Categorical Data\n",
    "df[categorical_columns[0]].value_counts().plot(kind='bar')\n",
    "\n",
    "# Bivariate Analysis (2D KDE Plot)\n",
    "sns.kdeplot(x='num_col1', y='num_col2', data=df, cmap='coolwarm', shade=True)\n",
    "\n",
    "# Boxplot for Numerical and Categorical Data\n",
    "sns.boxplot(x='categorical_column', y='numerical_column', data=df)\n",
    "\n",
    "# Heatmap of Correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Cross Tabulation\n",
    "pd.crosstab(df['categorical_col1'], df['categorical_col2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfbd97-22db-41ff-82d3-547ffa86ffc3",
   "metadata": {},
   "source": [
    "### What is Feature Engineering?\n",
    "\n",
    "Feature Engineering is the process of using domain knowledge to extract or create new input features from raw data, improving the performance of machine learning models. It involves transforming data into formats that are better suited for algorithms, and it can greatly impact the accuracy and performance of the models.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Feature Scaling**: Adjusts numerical features to a standard scale, which is crucial for algorithms sensitive to feature magnitude, such as distance-based algorithms (e.g., k-NN, SVM, etc.).\n",
    "- **Feature Construction**: Involves creating new features by combining existing ones. This can capture important patterns that aren't explicitly present in the raw data.\n",
    "- **Feature Selection**: Reduces the dimensionality of the dataset by selecting the most important features, which can help improve model performance and reduce overfitting.\n",
    "- **Feature Extraction**: Derives new features from raw data through techniques like PCA, which can reduce redundancy in the data and highlight important patterns.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Feature Scaling**:\n",
    "   - Ensures that numerical data is on the same scale, which helps machine learning models perform better.\n",
    "   - Common techniques include:\n",
    "     - **Standardization**: Rescales features to have zero mean and unit variance.\n",
    "     - **Normalization**: Rescales features to a range of 0-1 using Min-Max scaling.\n",
    "\n",
    "2. **Feature Construction**:\n",
    "   - Combines or transforms existing features to create new ones, such as adding interaction terms, binning continuous data, or aggregating features.\n",
    "   - Example: Constructing \"total_expense\" from \"monthly_expense\" and \"number_of_months\".\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - Identifies the most important features for the model, reducing overfitting and improving generalization.\n",
    "   - Techniques include:\n",
    "     - **Correlation-based**: Removes highly correlated features.\n",
    "     - **Wrapper methods**: Evaluates feature subsets by training models.\n",
    "     - **Embedded methods**: Uses algorithms like Lasso to perform feature selection.\n",
    "\n",
    "4. **Feature Extraction**:\n",
    "   - Reduces dimensionality by extracting features that encapsulate the most variance in the data. Principal Component Analysis (PCA) is commonly used here.\n",
    "\n",
    "#### Things to Keep in Mind:\n",
    "- **Scaling**: Most ML algorithms assume features are on the same scale, especially distance-based models.\n",
    "- **Construction**: Ensure that the constructed features are meaningful and capture the right relationships.\n",
    "- **Selection**: Avoid using too many or too few features, as this can lead to overfitting or underfitting.\n",
    "- **Extraction**: Feature extraction can help in high-dimensional datasets where reducing the number of features is critical.\n",
    "\n",
    "---\n",
    "\n",
    "Proper feature engineering helps models learn from data more effectively, leading to improved predictions and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dac0d-2e49-4623-9d8a-cec4e95b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "normalized_data = minmax_scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Feature Construction (Creating a new feature from existing ones)\n",
    "df['total_expense'] = df['monthly_expense'] * df['number_of_months']\n",
    "\n",
    "# Feature Selection (Select K Best based on ANOVA F-value)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "selected_features = selector.fit_transform(df[numerical_columns], df['target'])\n",
    "\n",
    "# Feature Extraction (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e81d6-2373-4fbc-8370-0918b640d5f6",
   "metadata": {},
   "source": [
    "### Pandas Code Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350b0fb-3184-47ee-b05a-58c3d471c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values directly in the original DataFrame\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Fill missing values with 0 in the original DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Drop a specific column 'age' from the DataFrame\n",
    "df.drop('age', axis=1, inplace=True)\n",
    "\n",
    "# Sort the DataFrame by the 'salary' column\n",
    "df.sort_values(by='salary', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56c9ef-a5ef-485b-96e6-d6df900e9dcc",
   "metadata": {},
   "source": [
    "### Pandas Profiling\n",
    "\n",
    "Pandas Profiling is a powerful tool for automating Exploratory Data Analysis (EDA). It quickly generates a comprehensive report of your dataset, summarizing key statistics and identifying potential data quality issues. This tool is especially useful for getting an overview of the dataset, such as descriptive statistics, correlations, missing data, and data types, without manually writing multiple code blocks. It provides detailed visualizations of distributions, correlations, and interactions, saving time and effort during initial data exploration.\n",
    "\n",
    "**Why Use It?**  \n",
    "- Provides a **quick EDA** summary of the dataset, offering detailed insights into data distributions, missing values, correlations, and feature statistics.\n",
    "- Highlights data quality issues such as missing values, outliers, high cardinality, and duplicates.\n",
    "- **Efficient for initial data exploration**, making it easier to understand the structure and issues of the dataset before feature engineering or model building.\n",
    "  \n",
    "**Key Features**:\n",
    "- Overview of dataset, including total missing values, duplicates, and feature cardinality.\n",
    "- Visual summaries of **univariate distributions**, correlations, and missing data patterns.\n",
    "- Flags **warnings** for potential issues in the dataset, helping you detect outliers or problematic data.\n",
    "  \n",
    "**Keep in Mind**:\n",
    "- For large datasets, generating a profiling report can be slow.\n",
    "- Use this as an early step in data exploration to gain insights into data quality and potential areas for feature engineering or cleaning.\n",
    "- Be mindful of the report's sensitivity when working with private or sensitive data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a824a-d130-4d58-a9ad-60a78f76a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas profiling if it's not installed\n",
    "# pip install pandas-profiling\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "# Generate a pandas profiling report for your DataFrame\n",
    "profile = df.profile_report(title=\"Pandas Profiling Report\")\n",
    "\n",
    "# Display the report in a Jupyter Notebook\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "# Optionally, save the report to an HTML file for further analysis\n",
    "profile.to_file(\"output_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4bb23-ad4a-44fa-adc2-9cb82ef92bc2",
   "metadata": {},
   "source": [
    "### Feature Scaling - Standardization\n",
    "\n",
    "**Standardization** (also known as Z-score normalization or mean centering) is a scaling technique where the features of the data are transformed such that they have a **mean of 0** and a **standard deviation of 1**. This is especially important for algorithms that are sensitive to the scale of the data, as it ensures that all features contribute equally during model training.\n",
    "\n",
    "**Why Use It?**  \n",
    "Some machine learning algorithms are sensitive to the scale of features, and it is important to normalize the range of all features to ensure they have comparable contributions to the model. Algorithms like **KMeans**, **K Nearest Neighbours (KNN)**, **PCA (Principal Component Analysis)**, and **Artificial Neural Networks** are particularly sensitive to the scales of the input data.\n",
    "\n",
    "**Use Cases**:\n",
    "- **KMeans Clustering**: Distance-based algorithms like KMeans require features to be on the same scale.\n",
    "- **KNN (K-Nearest Neighbors)**: For distance calculations in KNN, all features should be scaled appropriately.\n",
    "- **PCA**: Ensures that the principal components are influenced equally by all features.\n",
    "- **Neural Networks**: Helps speed up training and avoids certain features dominating the learning process due to larger magnitude.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Standardization works well with normally distributed features.\n",
    "- It can sometimes distort the importance of features that do not need to be scaled. Only use it for models that require features to be on the same scale.\n",
    "- Apply **standardization** only on the training data, and later use the same scaling factors for the test data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same transformation on test data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1a2d3-9485-4cef-984d-43fc688a0392",
   "metadata": {},
   "source": [
    "### Feature Scaling - Normalization | MinMaxScaling | MaxAbsScaling | RobustScaling\n",
    "\n",
    "**Normalization** involves rescaling the features of your dataset to a specific range, typically [0, 1] or [-1, 1], depending on the scaling technique. This is essential for machine learning models that are sensitive to the magnitude of input features.\n",
    "\n",
    "**Why Use It?**  \n",
    "Many machine learning algorithms like **logistic regression**, **support vector machines (SVM)**, and **neural networks** can perform better when features are on the same scale. Different scaling techniques suit different types of data and tasks.\n",
    "\n",
    "**Types of Scaling**:\n",
    "- **MinMaxScaling**: Scales features to a given range, usually [0, 1]. Best suited for algorithms that assume the features are bounded within a specific range.\n",
    "- **MaxAbsScaling**: Scales features by their maximum absolute value, leaving the sign unchanged. Ideal for data that is already centered at zero but needs rescaling.\n",
    "- **RobustScaling**: Uses the median and interquartile range (IQR) for scaling, making it more robust to outliers compared to MinMaxScaling.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- **Fit** the scaler on the **training data only** to avoid data leakage.\n",
    "- **Transform** both training and test datasets with the same scaler to ensure consistency.\n",
    "- MinMaxScaling can be sensitive to outliers, while RobustScaling handles them better.\n",
    "  \n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "min_max_scaler = MinMaxScaler()\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit on the training data only and transform both train and test data\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9657e6-e69f-42ad-a74a-816625bba7b9",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data | Ordinal Encoding | Label Encoding\n",
    "\n",
    "**Categorical Encoding** is essential when dealing with features that contain categorical data, as most machine learning algorithms expect numerical input. Two common types of encoding are **Ordinal Encoding** and **Label Encoding**.\n",
    "\n",
    "**Why Use It?**\n",
    "Machine learning algorithms cannot handle raw categorical data, and encoding is necessary to convert these categories into numerical values that models can understand.\n",
    "\n",
    "#### Types of Encoding:\n",
    "- **Ordinal Encoding**: This encoding is used when the categorical variable has an inherent order (e.g., \"low\", \"medium\", \"high\"). Each category is assigned an integer based on the rank.\n",
    "- **Label Encoding**: Assigns a unique integer to each category, with no inherent order. This is useful for nominal data, where no ordering exists.\n",
    "\n",
    "**Use Cases**:\n",
    "- **Ordinal Encoding** is suitable for ordinal features with meaningful rankings.\n",
    "- **Label Encoding** is generally used for nominal features (without order) but can introduce issues with algorithms that assume some ordering from integer values.\n",
    "\n",
    "**Column Transformer**: This is used when you need to apply different preprocessing techniques (like encoding or scaling) to different columns of your dataset. It is especially useful for pipelines with both categorical and numerical features.\n",
    "\n",
    "**Keep in Mind**:\n",
    "- Ordinal encoding should only be applied when the order of categories matters.\n",
    "- Label encoding can mislead some models if there is no inherent ordering but numbers are assigned. Consider **One-Hot Encoding** for such cases.\n",
    "- Fit the encoder on the training data and transform both train and test datasets.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "X = [['low'], ['medium'], ['high']]\n",
    "\n",
    "# Ordinal Encoding (for ordinal features)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "X_ordinal_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# Label Encoding (for nominal features)\n",
    "label_encoder = LabelEncoder()\n",
    "y = ['cat', 'dog', 'mouse']\n",
    "y_label_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Column Transformer - Example (if needed)\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(), ['categorical_column']),\n",
    "    ('num', 'passthrough', ['numerical_column'])\n",
    "])\n",
    "\n",
    "# Apply the transformations to the dataset\n",
    "X_transformed = column_transformer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42139ed6-d455-4d88-a308-1763bfd83b60",
   "metadata": {},
   "source": [
    "### One Hot Encoding | Handling Categorical Data\n",
    "\n",
    "**One-Hot Encoding** is a technique used to convert categorical data into a binary (0 or 1) matrix format. Each unique category is transformed into a separate column, where the presence of a category is marked as 1 and the absence as 0.\n",
    "\n",
    "**Why Use It?**  \n",
    "Unlike **Label Encoding**, which can introduce a false ordinal relationship between categories, One-Hot Encoding ensures that no such ordering is implied. It's particularly useful for nominal categorical data where categories have no order.\n",
    "\n",
    "#### Multicollinearity Concern:\n",
    "- **Multicollinearity** can arise when the columns created by one-hot encoding are not independent. To avoid this, we drop one column from the set of n categories (thus creating n-1 columns). This helps avoid the **Dummy Variable Trap**, where the presence of a category is perfectly predicted by the other categories.\n",
    "\n",
    "**Key Points to Keep in Mind**:\n",
    "- Use **One-Hot Encoding** when categories are **nominal** and there's no natural order.\n",
    "- Always ensure input columns are **independent** after encoding.\n",
    "- For categorical features with n categories, One-Hot Encoding will create **n-1 columns** to avoid multicollinearity.\n",
    "- Avoid the dummy variable trap by dropping one category from the encoded matrix.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi']\n",
    "})\n",
    "\n",
    "# One-Hot Encoding using pandas\n",
    "# We use drop_first=True to avoid the Dummy Variable Trap (n-1 columns)\n",
    "df_encoded = pd.get_dummies(df, columns=['City'], drop_first=True)\n",
    "\n",
    "# Example output would be:\n",
    "#    City_Lahore  City_Islamabad  City_Karachi\n",
    "# 0            0               0             1\n",
    "# 1            1               0             0\n",
    "# 2            0               1             0\n",
    "# 3            0               0             1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a32bb-e303-42a0-8973-e51f45d40cb4",
   "metadata": {},
   "source": [
    "### Column Transformer in Machine Learning\n",
    "\n",
    "The **Column Transformer** is a useful tool in machine learning when you need to apply different preprocessing techniques to different types of features (columns). For instance, you might want to apply **scaling** to numerical data while applying **encoding** to categorical data simultaneously.\n",
    "\n",
    "**Why Use It?**  \n",
    "It streamlines the preprocessing of datasets that contain both numerical and categorical variables, enabling you to handle each column type appropriately without manually applying transformations to each column.\n",
    "\n",
    "#### Key Use Cases:\n",
    "- **Numerical columns**: You may want to scale or normalize these features.\n",
    "- **Categorical columns**: You can apply encoders such as **One-Hot Encoding** or **Ordinal Encoding**.\n",
    "- **Pipelines**: The Column Transformer works well in conjunction with machine learning pipelines for streamlined workflows.\n",
    "\n",
    "#### Important Points to Consider:\n",
    "- Ensure the right transformation is applied to the correct column type.\n",
    "- You can chain transformers together to process different columns efficiently.\n",
    "- It's essential to fit the column transformer on training data and then apply it to both training and test sets to avoid **data leakage**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 32, 40, 50],\n",
    "    'city': ['Karachi', 'Lahore', 'Islamabad', 'Lahore'],\n",
    "    'salary': [50000, 60000, 65000, 70000]\n",
    "})\n",
    "\n",
    "# Column Transformer setup\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), ['age', 'salary']),  # Scaling numerical data\n",
    "    ('cat', OneHotEncoder(drop='first'), ['city'])  # One-Hot Encoding categorical data\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = column_transformer.fit_transform(df)\n",
    "\n",
    "# The numerical columns are scaled, and the categorical column is one-hot encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77278-f3ec-4544-9e86-71bf03cd87ed",
   "metadata": {},
   "source": [
    "### Function Transformer | Log Transform | Reciprocal Transform | Square Root Transform\n",
    "\n",
    "**Function Transformers** allow you to apply mathematical transformations to your features. Some of the common transformations include:\n",
    "- **Log Transform**: Helps handle skewed data by reducing the effect of large outliers. Used when data is highly right-skewed.\n",
    "- **Reciprocal Transform**: Similar to log transform but stronger. Works well with extreme values.\n",
    "- **Square Root Transform**: Less aggressive than log, used for moderately skewed data.\n",
    "\n",
    "These transformations are typically used when you want to make non-normally distributed data more Gaussian (normal), which helps improve model performance for algorithms that assume normality (e.g., linear regression, SVM).\n",
    "\n",
    "#### How to Check if Distribution is Normal?\n",
    "\n",
    "1. **sns.distplot()**: This plots the distribution of your data.\n",
    "   - If the plot shows a bell-shaped curve, the data is normally distributed.\n",
    "   \n",
    "2. **pd.skew()**: Returns the skewness of the data. \n",
    "   - If the skew is close to 0, the data is symmetric. \n",
    "   - Positive values indicate right-skewed data, and negative values indicate left-skewed data.\n",
    "   \n",
    "3. **QQ Plot**: (Quantile-Quantile plot) checks the normality by comparing the quantiles of your data against a theoretical normal distribution.\n",
    "   - If the points lie on the diagonal line, the data is normally distributed.\n",
    "\n",
    "#### Key Points:\n",
    "- **Log Transform**: Use for right-skewed data.\n",
    "- **Reciprocal Transform**: Use for data with extreme values.\n",
    "- **Square Root Transform**: Use for moderately skewed data.\n",
    "- Checking the data's distribution is crucial before applying transformations.\n",
    "- After transforming, always recheck the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'income': [1000, 3000, 10000, 50000, 100000, 200000]\n",
    "})\n",
    "\n",
    "# Checking Distribution with Seaborn's distplot\n",
    "sns.distplot(df['income'])\n",
    "plt.show()\n",
    "\n",
    "# Checking Skewness\n",
    "print(f\"Skewness: {df['income'].skew()}\")\n",
    "\n",
    "# QQ Plot to check normality\n",
    "stats.probplot(df['income'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# Applying Log Transform to reduce skewness\n",
    "df['income_log'] = np.log(df['income'] + 1)\n",
    "\n",
    "# Checking the transformed distribution\n",
    "sns.distplot(df['income_log'])\n",
    "plt.show()\n",
    "\n",
    "# Checking skewness after transformation\n",
    "print(f\"Skewness after log transform: {df['income_log'].skew()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a899ab5-2953-4e64-8203-0d80505e8d39",
   "metadata": {},
   "source": [
    "### Power Transformer | Box-Cox Transform | Yeo-Johnson Transform\n",
    "\n",
    "**Power Transformations** are used to make data more Gaussian-like by stabilizing variance and minimizing skewness. They are particularly useful when dealing with **heteroscedasticity** or non-normal distributions in features.\n",
    "\n",
    "#### **Box-Cox Transform**:\n",
    "- Requires input data to be **strictly positive**.\n",
    "- A parameter, **lambda**, controls the transformation (range from -5 to 5).\n",
    "- It works by finding an optimal lambda value that minimizes skewness.\n",
    "  \n",
    "#### **Yeo-Johnson Transform**:\n",
    "- Similar to Box-Cox but works with **both positive and negative** values.\n",
    "- More flexible than Box-Cox, making it suitable for a wider range of data distributions.\n",
    "  \n",
    "#### **Power Transformer**:\n",
    "- A generalized technique that includes Box-Cox and Yeo-Johnson transformations.\n",
    "- Automatically chooses the most appropriate transformation and finds the best lambda to make data more Gaussian.\n",
    "\n",
    "#### Key Points:\n",
    "- **Yeo-Johnson** is better because it handles negative values and zeroes.\n",
    "- **Box-Cox** is restricted to positive data only.\n",
    "- **Power Transformer** can handle different data ranges and provides more flexibility with both **Yeo-Johnson** and **Box-Cox** under its umbrella.\n",
    "- After transforming the data, always verify whether the transformation was successful using skewness, distplot, or QQ plots.\n",
    "\n",
    "#### When to Use:\n",
    "- Use these transformations when data is heavily skewed and you need to normalize it for better model performance, particularly for algorithms like linear regression, which assume normality.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with negative and positive values\n",
    "df = pd.DataFrame({'income': [-1000, 2000, 5000, 10000, 15000, 50000, 100000]})\n",
    "\n",
    "# Visualizing original distribution\n",
    "sns.distplot(df['income'])\n",
    "plt.title(\"Original Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Applying Power Transformer (Yeo-Johnson by default)\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "df['income_transformed'] = power_transformer.fit_transform(df[['income']])\n",
    "\n",
    "# Visualizing transformed distribution\n",
    "sns.distplot(df['income_transformed'])\n",
    "plt.title(\"Transformed Distribution (Yeo-Johnson)\")\n",
    "plt.show()\n",
    "\n",
    "# Checking the lambda value (for reference)\n",
    "print(f\"Lambda used for transformation: {power_transformer.lambdas_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1491c4-2bcf-4c8c-935f-4b425a8dfb25",
   "metadata": {},
   "source": [
    "### Binning and Binarization | Discretization | Quantile Binning | KMeans Binning\n",
    "\n",
    "**Binning** and **Binarization** are techniques to discretize continuous features into discrete categories or binary values, helping with:\n",
    "- Simplifying data interpretation\n",
    "- Handling outliers\n",
    "- Making data more robust for certain algorithms\n",
    "\n",
    "#### Types of Binning:\n",
    "\n",
    "1. **Equal Width Binning**:\n",
    "   - Divides the data into bins of equal width. Each bin covers the same range of values.\n",
    "   - Use when data is uniformly distributed.\n",
    "\n",
    "2. **Equal Frequency Binning**:\n",
    "   - Divides the data so that each bin contains approximately the same number of observations.\n",
    "   - Useful when the distribution is not uniform.\n",
    "\n",
    "3. **KMeans Binning**:\n",
    "   - Uses clustering (KMeans) to create bins based on natural groupings in the data.\n",
    "\n",
    "4. **Quantile Binning**:\n",
    "   - Divides data based on quantiles (percentiles). Each bin has approximately equal numbers of points, making it similar to equal-frequency binning but more statistically precise.\n",
    "\n",
    "#### Binarization:\n",
    "- Converts data into binary (0 or 1) values.\n",
    "- Often used for threshold-based classification or when converting continuous variables into categorical variables.\n",
    "\n",
    "#### Key Points:\n",
    "- **Equal Width** is straightforward but doesn't handle skewed data well.\n",
    "- **Equal Frequency** helps when data is unevenly distributed.\n",
    "- **KMeans Binning** identifies natural groupings in data and is more data-driven.\n",
    "- **Quantile Binning** is useful for creating balanced bins across different ranges of data.\n",
    "- **Binarization** is used when you need to convert continuous features into a binary form.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer, Binarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample data\n",
    "data = np.array([22, 23, 45, 54, 65, 76, 88, 94, 100]).reshape(-1, 1)\n",
    "\n",
    "# Equal Width Binning (bins of equal size)\n",
    "equal_width_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "binned_data_width = equal_width_binning.fit_transform(data)\n",
    "\n",
    "# Equal Frequency Binning (each bin has equal number of points)\n",
    "equal_freq_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "binned_data_freq = equal_freq_binning.fit_transform(data)\n",
    "\n",
    "# KMeans Binning\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "data_kmeans_binned = kmeans.fit_predict(data)\n",
    "\n",
    "# Quantile Binning (similar to Equal Frequency but based on quantiles)\n",
    "quantile_binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "binned_data_quantile = quantile_binning.fit_transform(data)\n",
    "\n",
    "# Binarization (Converting data to binary based on threshold)\n",
    "binarizer = Binarizer(threshold=50)\n",
    "binarized_data = binarizer.fit_transform(data)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Equal Width Binning:\\n{binned_data_width}\")\n",
    "print(f\"Equal Frequency Binning:\\n{binned_data_freq}\")\n",
    "print(f\"KMeans Binning:\\n{data_kmeans_binned}\")\n",
    "print(f\"Quantile Binning:\\n{binned_data_quantile}\")\n",
    "print(f\"Binarized Data:\\n{binarized_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bce5b9-8e9d-4762-a010-60df9681b759",
   "metadata": {},
   "source": [
    "### Handling Mixed Variables | Feature Engineering\n",
    "\n",
    "**Mixed Variables** refer to datasets that contain both **numerical** and **categorical** data types. Effectively handling mixed variables is crucial for building robust machine learning models. The steps to deal with mixed variables include:\n",
    "\n",
    "1. **Identify Variable Types**:\n",
    "   - Understand the types of variables present in your dataset. Numerical variables can be continuous (e.g., income) or discrete (e.g., number of children), while categorical variables can be nominal (e.g., city names) or ordinal (e.g., satisfaction ratings).\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **Transform Categorical Variables**: Convert categorical variables into numerical formats using methods like One-Hot Encoding, Label Encoding, or Ordinal Encoding.\n",
    "   - **Scale Numerical Variables**: Apply feature scaling techniques like Standardization or Normalization to ensure numerical variables are on a similar scale.\n",
    "\n",
    "3. **Combine Features**:\n",
    "   - Create new features by combining existing numerical and categorical features. For instance, you could multiply a numerical feature by a categorical one (after encoding) to create an interaction feature.\n",
    "\n",
    "4. **Impute Missing Values**:\n",
    "   - Handle missing values differently based on variable type. For numerical variables, use methods like mean or median imputation; for categorical variables, consider the most frequent category or a new category to indicate missingness.\n",
    "\n",
    "5. **Outlier Handling**:\n",
    "   - Identify and address outliers in numerical features without affecting the categorical features.\n",
    "\n",
    "#### Important Considerations:\n",
    "- Ensure that transformations maintain the integrity of the data.\n",
    "- Be cautious about introducing multicollinearity when combining features.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Sample dataset with mixed variables\n",
    "data = {\n",
    "    'age': [22, 25, 27, 35, 45],\n",
    "    'city': ['Karachi', 'Lahore', 'Karachi', 'Lahore', 'Islamabad'],\n",
    "    'income': [50000, 60000, 80000, 75000, 90000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: One-Hot Encoding for Categorical Variables\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_city = encoder.fit_transform(df[['city']])\n",
    "encoded_df = pd.DataFrame(encoded_city, columns=encoder.get_feature_names_out(['city']))\n",
    "\n",
    "# Step 2: Scale Numerical Variables\n",
    "scaler = StandardScaler()\n",
    "scaled_income = scaler.fit_transform(df[['income']])\n",
    "scaled_income_df = pd.DataFrame(scaled_income, columns=['scaled_income'])\n",
    "\n",
    "# Combine encoded categorical and scaled numerical features\n",
    "final_df = pd.concat([df[['age']], encoded_df, scaled_income_df], axis=1)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889e67b-fdec-4c5f-9a3e-461a051e3e47",
   "metadata": {},
   "source": [
    "### Handling Missing Data | Part 1\n",
    "\n",
    "Handling missing data is an essential step in the data preprocessing phase. Missing data can arise due to various reasons, and it is crucial to handle it effectively to avoid biased results or errors during model training. There are several methods to deal with missing data, each suited for different situations.\n",
    "\n",
    "#### Techniques to Handle Missing Data:\n",
    "\n",
    "1. **Remove the Column**:\n",
    "   - If a feature (column) has a significant amount of missing data (e.g., > 60%), it may be better to drop that column, as it may not provide useful information for the model.\n",
    "   - **When to use**: If the column is not important for the analysis and the missing data is extensive, dropping the column can be an efficient solution.\n",
    "\n",
    "2. **Complete Case Analysis (CCA)**:\n",
    "   - CCA involves removing rows with any missing values in the dataset. This method assumes that the data is missing completely at random (MCAR), meaning the missingness is not related to any other feature or the outcome.\n",
    "   - **When to use**: If the missing data is sparse and missing completely at random, this method can be applied without significantly affecting the overall dataset.\n",
    "\n",
    "3. **Missing Completely at Random (MCAR)**:\n",
    "   - Data is considered MCAR when the missingness of data points does not depend on any observed or unobserved data.\n",
    "   - **Use case**: When MCAR is confirmed, CCA can be a simple and effective approach without introducing bias into the analysis.\n",
    "\n",
    "#### Considerations for CCA:\n",
    "- **When to apply CCA**:\n",
    "   - CCA is generally recommended when the proportion of missing data is small, and the missing data does not introduce bias into the dataset.\n",
    "   - If too many rows are removed, it may lead to a loss of valuable information, so use with caution.\n",
    "   \n",
    "---\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Age': [25, 30, None, 22, 35],\n",
    "    'Income': [50000, None, 60000, 65000, 70000],\n",
    "    'City': ['Karachi', None, 'Lahore', 'Islamabad', 'Lahore']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove column if too many missing values (example: if more than 60% of values are missing)\n",
    "if df['Income'].isnull().mean() > 0.6:\n",
    "    df = df.drop(columns=['Income'])\n",
    "\n",
    "# Step 2: Complete Case Analysis (Remove rows with any missing values)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display cleaned DataFrame\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3d286-fcb1-4aa2-ac15-9af494d8d15f",
   "metadata": {},
   "source": [
    "### Handling Missing Data | Numerical Data | Simple Imputer\n",
    "\n",
    "When handling missing numerical data, it's important to decide on the imputation technique based on the distribution of the data. Imputation is essential because many machine learning algorithms do not accept missing values, and removing rows or columns might lead to data loss. There are two primary types of imputation techniques:\n",
    "\n",
    "#### Types of Imputation:\n",
    "\n",
    "1. **Univariate Imputation**:\n",
    "   - In this approach, missing values in each column are imputed independently.\n",
    "   - Common methods include imputing with the mean, median, or a constant (arbitrary value).\n",
    "\n",
    "2. **Multivariate Imputation**:\n",
    "   - This method considers the relationships between different columns while imputing missing values, which can capture correlations between variables.\n",
    "\n",
    "#### Imputation Techniques for Numerical Data:\n",
    "\n",
    "- **Mean Imputation**:\n",
    "  - This method replaces missing values with the mean of the column.\n",
    "  - **When to use**: If the distribution of the data is almost normal, this is a reasonable approach.\n",
    "\n",
    "- **Median Imputation**:\n",
    "  - Replaces missing values with the median of the column.\n",
    "  - **When to use**: If the distribution is slightly skewed, the median is a better choice as it is less affected by outliers.\n",
    "\n",
    "- **Arbitrary Value Imputation**:\n",
    "  - Missing values are replaced with an arbitrary value such as 0, -999, etc.\n",
    "  - **Use case**: When the missing data itself may carry information or when imputing with a domain-specific value is required.\n",
    "\n",
    "- **End of Distribution Imputation**:\n",
    "  - This technique fills missing values with values at the extreme end of the distribution.\n",
    "  - **Use case**: When the data is skewed, and extreme values might signify something meaningful.\n",
    "\n",
    "- **Random Sample Imputation**:\n",
    "  - Missing values are imputed by randomly selecting from the available data points of the column.\n",
    "  - **Use case**: Helps preserve the distribution of the feature.\n",
    "\n",
    "#### Choosing between Mean and Median:\n",
    "- **Mean**: Use it when the distribution is almost normal (symmetric, bell-shaped curve).\n",
    "- **Median**: Use it when the distribution is skewed (i.e., has outliers).\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing numerical values\n",
    "data = {\n",
    "    'Age': [25, 30, None, 22, 35],\n",
    "    'Income': [50000, None, 60000, 65000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Use SimpleImputer for mean imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df['Age_mean_imputed'] = mean_imputer.fit_transform(df[['Age']])\n",
    "\n",
    "# Step 2: Use SimpleImputer for median imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df['Income_median_imputed'] = median_imputer.fit_transform(df[['Income']])\n",
    "\n",
    "# Display DataFrame after imputation\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb53bd-8731-4149-89d7-3547ebb1189b",
   "metadata": {},
   "source": [
    "### Handling Missing Categorical Data | Simple Imputer | Most Frequent Imputation | Missing Category Imputation\n",
    "\n",
    "When dealing with missing categorical data, it is essential to choose an imputation strategy that preserves the distribution and meaning of the data. Unlike numerical data, where mean or median might be used, categorical data is best handled with mode or frequent category imputation.\n",
    "\n",
    "#### Common Techniques for Categorical Data Imputation:\n",
    "\n",
    "1. **Most Frequent (Mode) Imputation**:\n",
    "   - The most common value (mode) in the column is used to replace the missing values.\n",
    "   - **Use case**: If the missing data is assumed to follow the distribution of the rest of the data, this method works well.\n",
    "\n",
    "2. **Missing Category Creation**:\n",
    "   - A new category is created to explicitly indicate missing data. This is useful when missing data itself may carry some information.\n",
    "   - **Use case**: This approach is useful when you do not want to guess the missing values and prefer to label them as 'missing' or some other placeholder.\n",
    "\n",
    "#### Key Points:\n",
    "- **Most Frequent (Mode) Imputation** is widely used for categorical data because categorical values usually have a \"majority class\" that is repeated often, making it a good candidate to replace missing values.\n",
    "- **Mean/Median** is only applicable to numerical data; for categorical data, **mode** or custom categories are better suited.\n",
    "- **Custom Category Creation**: Creating a specific \"missing\" label helps avoid potential biases that could occur if an incorrect mode is imputed.\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing categorical values\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', None, 'Female', 'Male'],\n",
    "    'Occupation': [None, 'Engineer', 'Doctor', 'Teacher', None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Most Frequent (Mode) Imputation\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['Gender_mode_imputed'] = mode_imputer.fit_transform(df[['Gender']])\n",
    "\n",
    "# Step 2: Creating a new category for missing data\n",
    "missing_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "df['Occupation_missing_imputed'] = missing_imputer.fit_transform(df[['Occupation']])\n",
    "\n",
    "# Display DataFrame after imputation\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a61b8-4ff2-4614-b34d-fc780ee3546a",
   "metadata": {},
   "source": [
    "### Missing Indicator | Random Sample Imputation | Handling Missing Data Part 4\n",
    "\n",
    "Handling missing data can be challenging, and certain techniques are used to either impute missing values or flag them for further analysis. Two common approaches include Random Sample Imputation and the use of a Missing Indicator.\n",
    "\n",
    "#### Techniques for Handling Missing Data:\n",
    "\n",
    "1. **Random Sample Imputation**:\n",
    "   - A random value from the non-missing entries of a feature is chosen to fill in the missing data.\n",
    "   - **Use case**: This method can be used to maintain the distribution of the feature without introducing a strong bias, but it may distort some patterns in the data.\n",
    "   \n",
    "2. **Missing Indicator**:\n",
    "   - Instead of replacing missing values, this technique flags which values are missing by creating a new feature column that indicates whether a value was missing (1) or present (0).\n",
    "   - **Use case**: It helps algorithms to learn if missing data is informative and can be used alongside other imputation strategies.\n",
    "   - **Important point**: Grid Search CV or other automatic model selection techniques may help determine if missing indicators improve the model's performance.\n",
    "\n",
    "#### Key Points:\n",
    "- **Random Sample Imputation**: Useful when the distribution of missing data should be preserved, but it can introduce randomness and may not always be reliable.\n",
    "- **Missing Indicator**: This method is important because missing data itself can be informative. It creates a flag for missingness, allowing the model to consider it during training.\n",
    "- **Model Selection**: Tools like Grid Search CV can be used to automatically select whether the missing indicator or random imputation improves the models performance.\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import MissingIndicator\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Age': [25, 35, None, 45, None],\n",
    "    'Salary': [50000, 60000, 55000, None, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Random Sample Imputation\n",
    "random_imputer = SimpleImputer(strategy='mean')\n",
    "df['Age_random_imputed'] = random_imputer.fit_transform(df[['Age']])\n",
    "\n",
    "# Step 2: Missing Indicator\n",
    "indicator = MissingIndicator(features='missing-only')\n",
    "missing_flags = indicator.fit_transform(df[['Age']])\n",
    "\n",
    "# Combine the missing indicator with the original data\n",
    "df['Age_missing_flag'] = missing_flags\n",
    "\n",
    "# Step 3: Automatic Model Selection (GridSearchCV Example)\n",
    "# Create a basic model (e.g., Random Forest) and search for the best parameters\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [3, 5, 7]}\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "clf.fit(df.drop(columns='Age'), missing_flags)\n",
    "\n",
    "# Display DataFrame with imputation and missing indicator\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b2d08-0f20-44cd-a345-392c9728cf2e",
   "metadata": {},
   "source": [
    "### KNN Imputer | Multivariate Imputation | Handling Missing Data Part 5\n",
    "\n",
    "**KNN Imputation** is a multivariate imputation method that uses the k-nearest neighbors to estimate and fill in the missing values based on the similarity of the observations. It is particularly useful when there is a relationship between different features that can be leveraged to predict the missing values.\n",
    "\n",
    "#### Techniques for Handling Missing Data:\n",
    "\n",
    "1. **KNN Imputer**:\n",
    "   - The K-Nearest Neighbors (KNN) algorithm is used to impute missing values by looking for 'k' observations (neighbors) with the most similar patterns of feature values.\n",
    "   - It calculates the distances (Euclidean or non-Euclidean) between samples and uses the nearest samples to fill in the missing data.\n",
    "   - **Use case**: Best used when you believe the relationships between features can help estimate missing values. KNN works well for both numerical and categorical variables.\n",
    "   - **Euclidean Distance**: This is the default distance metric used for continuous, numerical variables.\n",
    "   - **Non-Euclidean Distance**: Non-Euclidean metrics such as Manhattan distance can be used for categorical or non-linear data.\n",
    "\n",
    "2. **Multivariate Imputation**:\n",
    "   - Multivariate imputation considers multiple features simultaneously to estimate the missing values, making it more robust than univariate methods, especially when features are interdependent.\n",
    "   - **Use case**: This is preferred when there are strong correlations between features, and filling missing values in one feature depends on other features.\n",
    "\n",
    "#### Key Points:\n",
    "- **KNN Imputation**: Uses the 'k' most similar samples (neighbors) to impute missing values. Works well for structured datasets with a clear relationship between features.\n",
    "- **Euclidean Distance**: Suitable for continuous data, but alternative distance metrics (Manhattan, etc.) can be used for categorical or other data types.\n",
    "- **Multivariate Imputation**: It is more accurate than univariate methods when dealing with complex datasets with interdependent variables.\n",
    "\n",
    "#### Important Considerations:\n",
    "- KNN imputation can be computationally expensive for large datasets.\n",
    "- Choose the right distance metric (Euclidean vs. Non-Euclidean) based on the nature of your features (numerical vs. categorical).\n",
    "---\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Age': [25, 35, None, 45, None],\n",
    "    'Salary': [50000, 60000, 55000, None, 70000],\n",
    "    'Experience': [1, 5, 3, 7, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=3, metric='nan_euclidean')  # You can change the metric to 'manhattan' if needed\n",
    "df_imputed = knn_imputer.fit_transform(df)\n",
    "\n",
    "# Step 2: Convert back to DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "# Display DataFrame after KNN Imputation\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce095f-caa1-4293-a877-a00226f378a0",
   "metadata": {},
   "source": [
    "### Multivariate Imputation by Chained Equations for Missing Value | MICE Algorithm | Iterative Imputer\n",
    "\n",
    "The **Multivariate Imputation by Chained Equations (MICE)** algorithm, also called **Iterative Imputer**, is a powerful method for handling missing data. It works by modeling each feature with missing values as a function of the other features, iteratively imputing missing data multiple times to generate a robust estimate.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **MICE Algorithm**:\n",
    "   - The MICE algorithm iteratively models each feature with missing values by regressing it against the other features in the dataset.\n",
    "   - It assumes that the missing values in each variable can be predicted from the others, and it cycles through the variables several times, improving the imputed values in each iteration.\n",
    "   - **Use case**: Suitable when you have interdependent features, and you believe that missing values in one feature can be predicted by others.\n",
    "\n",
    "2. **Types of Missing Data**:\n",
    "   - **MCAR (Missing Completely At Random)**: Missingness does not depend on the data itself (e.g., data lost during collection). This is ideal for imputation since no pattern exists in the missingness.\n",
    "   - **MAR (Missing At Random)**: Missingness is related to observed data but not the missing data itself. For example, a survey respondents income might be missing depending on their age group.\n",
    "   - **MNAR (Missing Not At Random)**: The missing data depends on the unseen (missing) data itself, like people not reporting their income due to its high value. This type of missing data is more challenging to handle because it introduces bias.\n",
    "\n",
    "#### Steps in MICE Algorithm:\n",
    "1. Initialize missing values with random guesses.\n",
    "2. For each variable with missing data, fit a regression model using the other variables to predict the missing values.\n",
    "3. Replace the missing values with the predictions from the model.\n",
    "4. Repeat the process for a fixed number of iterations (convergence).\n",
    "5. After imputation, the imputed data can be used for analysis.\n",
    "\n",
    "#### Important Considerations:\n",
    "- **MICE** can handle both continuous and categorical data, but it's important to specify the right model for each feature type.\n",
    "- The imputation process assumes that the relationships between features are strong enough to predict missing values, so use MICE when you have correlated features.\n",
    "- The model converges iteratively, so more iterations can lead to better results.\n",
    "\n",
    "#### MCAR, MAR, MNAR in MICE:\n",
    "- **MCAR**: Missingness is truly random, and MICE can work well without introducing bias.\n",
    "- **MAR**: MICE works well under the assumption that missing data depends on observed data.\n",
    "- **MNAR**: Requires careful consideration, as imputing values can introduce bias, and MICE may not be suitable.\n",
    "\n",
    "---\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Age': [25, 35, None, 45, None],\n",
    "    'Salary': [50000, 60000, 55000, None, 70000],\n",
    "    'Experience': [1, 5, 3, 7, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Iterative Imputation using MICE Algorithm\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=0)  # max_iter controls the number of iterations\n",
    "df_imputed = mice_imputer.fit_transform(df)\n",
    "\n",
    "# Step 2: Convert back to DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "# Display DataFrame after MICE Imputation\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7969f-06ef-4477-b0b0-53f720b5d9b9",
   "metadata": {},
   "source": [
    "### What are Outliers | Outliers in Machine Learning\n",
    "\n",
    "**Outliers** are data points that differ significantly from other observations in a dataset. They can occur due to variability in the data, errors, or rare events. Identifying and handling outliers is crucial, as they can skew results and negatively impact machine learning models.\n",
    "\n",
    "#### Importance of Detecting Outliers:\n",
    "- Outliers can significantly affect the performance of machine learning algorithms, especially those based on distance measures or that assume normal distribution.\n",
    "- If not handled properly, outliers can cause inaccurate predictions, biased models, and high variance.\n",
    "\n",
    "#### Which Algorithms Are Impacted by Outliers:\n",
    "- **Impacted Algorithms**: \n",
    "  - Algorithms like **Linear Regression**, **Logistic Regression**, **K-Nearest Neighbors (KNN)**, **K-means Clustering**, and **Principal Component Analysis (PCA)** are sensitive to outliers. Outliers can distort the models understanding of the underlying data distribution.\n",
    "- **Less Impacted Algorithms**:\n",
    "  - Tree-based models like **Random Forest** and **Decision Trees** are more robust to outliers as they split based on criteria rather than distances.\n",
    "\n",
    "#### Methods for Handling Outliers:\n",
    "1. **Trimming**:\n",
    "   - Remove outliers entirely from the dataset. This method works well when youre certain the outliers are anomalies and not valuable data points.\n",
    "   - **Use case**: Useful when outliers are errors or rare, and their presence adds noise.\n",
    "\n",
    "2. **Capping (Winsorizing)**:\n",
    "   - Set a threshold and replace extreme outliers with the nearest acceptable value (capping the values). This is a way to reduce the influence of extreme values without removing them.\n",
    "   - **Use case**: Preferred when outliers may still hold valuable information but need to be reduced in their impact.\n",
    "\n",
    "3. **Treat Outliers as Missing**:\n",
    "   - Outliers can be treated as missing data points and then imputed using techniques like mean, median, or model-based imputation.\n",
    "   - **Use case**: If an outlier is detected but not enough data exists to make conclusive assumptions, imputing missing values can mitigate bias.\n",
    "\n",
    "4. **Discretization**:\n",
    "   - Group values into bins or categories to reduce the influence of outliers.\n",
    "   - **Use case**: This method is useful for converting continuous outlier-prone features into categories, making models more robust.\n",
    "\n",
    "#### Techniques for Detecting Outliers:\n",
    "1. **Z-score Method**:\n",
    "   - Measures how many standard deviations a data point is from the mean.\n",
    "   - Typically, a Z-score of 3 is considered an outlier.\n",
    "   \n",
    "2. **IQR (Interquartile Range) Method**:\n",
    "   - Calculates the range between the 1st and 3rd quartiles (Q1 and Q3).\n",
    "   - Any data point outside the range `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]` is considered an outlier.\n",
    "   \n",
    "3. **Percentile Method**:\n",
    "   - Use percentiles to determine outliers and cap them based on the lower and upper thresholds (e.g., 1st and 99th percentiles).\n",
    "\n",
    "#### Important Considerations:\n",
    "- **When to Keep Outliers**: If they represent rare but valid cases, like anomalies that might be the key to predictive modeling (e.g., fraud detection).\n",
    "- **When to Remove Outliers**: If they are errors in data entry or collection or don't contribute meaningfully to the predictive power of the model.\n",
    "---\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample DataFrame with potential outliers\n",
    "data = {\n",
    "    'Age': [25, 35, 30, 28, 100, 27, 32],  # Age 100 is likely an outlier\n",
    "    'Salary': [50000, 60000, 55000, 62000, 1200000, 58000, 60500]  # Salary 1200000 is an outlier\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Z-Score Method\n",
    "z_scores = np.abs(stats.zscore(df))  # Compute Z-scores\n",
    "outliers_zscore = np.where(z_scores > 3)  # Find outliers where Z-score > 3\n",
    "print(\"Z-Score Outliers:\\n\", df.iloc[outliers_zscore[0]])\n",
    "\n",
    "# 2. IQR Method\n",
    "Q1 = df.quantile(0.25)  # 1st Quartile\n",
    "Q3 = df.quantile(0.75)  # 3rd Quartile\n",
    "IQR = Q3 - Q1  # Interquartile Range\n",
    "outliers_iqr = df[~((df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(\"IQR Outliers:\\n\", outliers_iqr)\n",
    "\n",
    "# 3. Capping Outliers (Winsorizing)\n",
    "df_capped = df.copy()\n",
    "df_capped['Salary'] = np.where(df_capped['Salary'] > 100000, 100000, df_capped['Salary'])  # Cap salaries > 100k\n",
    "print(\"Capped Data:\\n\", df_capped)\n",
    "\n",
    "# 4. Remove Outliers\n",
    "df_trimmed = df[(z_scores < 3).all(axis=1)]  # Remove rows with Z-scores greater than 3\n",
    "print(\"Trimmed Data:\\n\", df_trimmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8249330-705e-47e0-b845-cc55e9b87207",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "Regression metrics are essential for evaluating the performance of regression models. The most commonly used metrics include Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared (R), and Adjusted R-squared. Understanding these metrics helps in determining how well your model predicts the target variable.\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "- **Definition**: MSE measures the average of the squares of the errors, which is the difference between the actual and predicted values.\n",
    "- **Use Case**: Used when you want to penalize larger errors more severely.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
    "  \\]\n",
    "- **Interpretation**: Lower values are better, with a perfect score of 0 indicating no error.\n",
    "\n",
    "### 2. Mean Absolute Error (MAE)\n",
    "- **Definition**: MAE measures the average absolute differences between predicted and actual values.\n",
    "- **Use Case**: Used when you want to treat all errors equally.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|\n",
    "  \\]\n",
    "- **Interpretation**: Lower values are better, with a perfect score of 0 indicating no error.\n",
    "\n",
    "### 3. Root Mean Squared Error (RMSE)\n",
    "- **Definition**: RMSE is the square root of the MSE and provides an estimation of the standard deviation of the prediction errors.\n",
    "- **Use Case**: Useful for understanding the magnitude of errors in the same units as the target variable.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}\n",
    "  \\]\n",
    "- **Interpretation**: Lower values are better, with a perfect score of 0 indicating no error.\n",
    "\n",
    "### 4. R-squared (R)\n",
    "- **Definition**: R is a statistical measure that represents the proportion of variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
    "- **Use Case**: Helps in understanding the goodness of fit of the model.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\text{SS}_{res}}{\\text{SS}_{tot}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  \\]\n",
    "- **Interpretation**: R values range from 0 to 1; closer to 1 indicates a better fit.\n",
    "\n",
    "### 5. Adjusted R-squared\n",
    "- **Definition**: Adjusted R adjusts the R value for the number of predictors in the model, providing a more accurate measure for models with multiple predictors.\n",
    "- **Use Case**: Helps in comparing models with different numbers of predictors.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{Adjusted } R^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n",
    "  \\]\n",
    "  where \\( p \\) is the number of predictors.\n",
    "- **Interpretation**: Adjusted R can be lower than R; it helps in model selection.\n",
    "\n",
    "### Criteria for Judging Metrics\n",
    "- **MSE**: The closer to 0, the better the model.\n",
    "- **MAE**: A smaller MAE indicates better performance.\n",
    "- **RMSE**: Should be small relative to the range of the target variable.\n",
    "- **R**: Values closer to 1 indicate a good fit; values <0 indicate a poor model.\n",
    "- **Adjusted R**: Useful for comparing models; higher values are preferred, especially with more predictors.\n",
    "\n",
    "### Summary\n",
    "In summary, these metrics provide insights into the accuracy and performance of regression models, helping you to assess how well the model is performing and where it might need improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca81d80-fd28-4f88-91ff-f98da2db9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assume y_true and y_pred are your actual and predicted values\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R^2: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc363a-0874-4524-b6af-386a3b8de728",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-off\n",
    "\n",
    "The **bias-variance trade-off** is a fundamental concept in machine learning that describes the balance between two types of errors that affect the performance of predictive models: bias and variance. Understanding this trade-off helps in selecting the right model and improving its predictive performance.\n",
    "\n",
    "### 1. Bias\n",
    "- **Definition**: Bias is the error introduced by approximating a real-world problem with a simplified model. It reflects how closely the model can fit the training data.\n",
    "- **Characteristics**:\n",
    "  - **High Bias**: The model makes strong assumptions about the data, leading to oversimplification. It fails to capture the underlying patterns, resulting in **underfitting**.\n",
    "  - **Low Bias**: The model makes fewer assumptions, which allows it to fit the data better.\n",
    "\n",
    "### 2. Variance\n",
    "- **Definition**: Variance is the error introduced by the model's sensitivity to fluctuations in the training data. It reflects how much the model's predictions vary for different training sets.\n",
    "- **Characteristics**:\n",
    "  - **High Variance**: The model captures noise along with the underlying patterns in the training data, leading to **overfitting**.\n",
    "  - **Low Variance**: The model is more stable and generalizes better to unseen data.\n",
    "\n",
    "### 3. Overfitting\n",
    "- **Definition**: Overfitting occurs when a model learns the training data too well, including its noise and outliers. This results in excellent performance on training data but poor performance on unseen test data.\n",
    "- **Characteristics**:\n",
    "  - **Signs of Overfitting**: High accuracy on training data, low accuracy on test data.\n",
    "  - **Cause**: Often due to a complex model with many parameters relative to the number of training examples.\n",
    "\n",
    "### 4. Underfitting\n",
    "- **Definition**: Underfitting occurs when a model is too simple to capture the underlying patterns of the data. It leads to poor performance on both training and test data.\n",
    "- **Characteristics**:\n",
    "  - **Signs of Underfitting**: Low accuracy on both training and test data.\n",
    "  - **Cause**: Often due to a model that is too simplistic, with insufficient parameters.\n",
    "\n",
    "### 5. Target: Low Bias, Low Variance\n",
    "- The ideal scenario is to achieve a model that has both low bias and low variance, resulting in good performance on both training and test datasets.\n",
    "\n",
    "### 6. Strategies to Achieve Low Bias and Low Variance\n",
    "To achieve a balance in the bias-variance trade-off, various strategies can be employed:\n",
    "\n",
    "- **Regularization**: Techniques like L1 (Lasso) and L2 (Ridge) regularization add penalties to the loss function, discouraging overly complex models.\n",
    "- **Bagging**: This ensemble technique helps reduce variance by training multiple models on different subsets of the training data and averaging their predictions (e.g., Random Forest).\n",
    "- **Boosting**: Another ensemble technique that builds models sequentially, where each new model tries to correct the errors of the previous one, reducing both bias and variance.\n",
    "\n",
    "### Summary\n",
    "Understanding the bias-variance trade-off is crucial in model selection and tuning. The goal is to find a model that minimizes both bias and variance, thereby improving generalization to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf8b53-b719-403e-bafc-bbfb7747888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Regularization, Bagging, and Boosting\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.random.rand(100, 10)  # Features\n",
    "y = np.random.rand(100)       # Target variable\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Regularization (Ridge)\n",
    "ridge = Ridge(alpha=1.0)  # L2 regularization\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "# Bagging (Random Forest)\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# Boosting (Gradient Boosting)\n",
    "gb = GradientBoostingRegressor(n_estimators=100)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "gb_mse = mean_squared_error(y_test, gb_pred)\n",
    "\n",
    "print(f'Ridge MSE: {ridge_mse}')\n",
    "print(f'Random Forest MSE: {rf_mse}')\n",
    "print(f'Gradient Boosting MSE: {gb_mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df4ea0-d29f-445d-9d20-c01cd5d5dced",
   "metadata": {},
   "source": [
    "## Accuracy and Confusion Matrix\n",
    "\n",
    "In classification problems, the **accuracy** metric measures the proportion of correct predictions made by the model out of all predictions. However, accuracy alone can be misleading, especially when dealing with imbalanced datasets.\n",
    "\n",
    "### 1. Confusion Matrix\n",
    "A confusion matrix is a tabular representation of actual versus predicted classifications. It provides insights into the performance of a classification model.\n",
    "\n",
    "- **Components**:\n",
    "  - **True Positive (TP)**: Correctly predicted positive cases.\n",
    "  - **True Negative (TN)**: Correctly predicted negative cases.\n",
    "  - **False Positive (FP)**: Incorrectly predicted positive cases (Type I error).\n",
    "  - **False Negative (FN)**: Incorrectly predicted negative cases (Type II error).\n",
    "\n",
    "### 2. Type I and Type II Errors\n",
    "- **Type I Error (False Positive)**: Rejecting a true null hypothesis. In terms of classification, this is when a model incorrectly predicts the positive class.\n",
    "- **Type II Error (False Negative)**: Failing to reject a false null hypothesis. In classification, this occurs when a model incorrectly predicts the negative class.\n",
    "\n",
    "### 3. Accuracy Formula\n",
    "The formula for calculating accuracy is:\n",
    "\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "\n",
    "### Summary\n",
    "The confusion matrix helps to understand the nuances of model performance beyond just accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision, Recall, and F1 Score\n",
    "\n",
    "When evaluating classification models, especially with imbalanced classes, additional metrics such as precision, recall, and the F1 score provide more meaningful insights into model performance.\n",
    "\n",
    "### 1. Precision\n",
    "- **Definition**: Precision is the ratio of correctly predicted positive observations to the total predicted positives.\n",
    "- **Formula**:\n",
    "\\[ \\text{Precision} = \\frac{TP}{TP + FP} \\]\n",
    "- **Use Case**: High precision is important in cases where the cost of false positives is high.\n",
    "\n",
    "### 2. Recall (Sensitivity)\n",
    "- **Definition**: Recall is the ratio of correctly predicted positive observations to all actual positives.\n",
    "- **Formula**:\n",
    "\\[ \\text{Recall} = \\frac{TP}{TP + FN} \\]\n",
    "- **Use Case**: High recall is crucial in scenarios where missing a positive case is costly, such as in medical diagnoses.\n",
    "\n",
    "### 3. F1 Score\n",
    "- **Definition**: The F1 score is the harmonic mean of precision and recall. It balances the two metrics, especially in cases of class imbalance.\n",
    "- **Formula**:\n",
    "\\[ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "- **Use Case**: The F1 score is particularly useful when you need a balance between precision and recall.\n",
    "\n",
    "### Summary\n",
    "Using precision, recall, and F1 score allows for a more comprehensive evaluation of classification model performance, especially when dealing with imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125e819-663b-4976-a3e3-d4bf3c023251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Calculating Metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Sample true and predicted labels\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc109d5-6a28-4da4-b56f-ae9433725c4a",
   "metadata": {},
   "source": [
    "## Introduction to Ensemble Learning\n",
    "\n",
    "**Ensemble Learning** is a machine learning paradigm where multiple models (often referred to as \"weak learners\") are combined to produce a stronger overall model. The idea is to leverage the strengths of various models while compensating for their weaknesses, ultimately leading to improved performance in terms of accuracy and robustness.\n",
    "\n",
    "### Key Concepts\n",
    "- **Base Models**: The individual models that make up the ensemble. They should ideally be different from each other to ensure diversity.\n",
    "- **Diversity**: The concept that different models will make different errors. By combining their predictions, we can often achieve better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Ensemble Techniques in Machine Learning\n",
    "\n",
    "### 1. Voting Ensemble\n",
    "Voting ensemble methods combine the predictions of multiple models by using majority voting (for classification) or averaging (for regression).\n",
    "\n",
    "- **Types**:\n",
    "  - **Hard Voting**: Each model votes for a class, and the class with the majority of votes is selected.\n",
    "  - **Soft Voting**: Models output probabilities for each class, and the class with the highest average probability is selected.\n",
    "\n",
    "### 2. Bagging (Bootstrap Aggregating)\n",
    "Bagging is an ensemble technique that reduces variance by training multiple base models on different random samples of the data (with replacement). The predictions from these models are then averaged (for regression) or voted on (for classification).\n",
    "\n",
    "- **Key Points**:\n",
    "  - Reduces overfitting.\n",
    "  - Example: Random Forest, which combines multiple decision trees.\n",
    "\n",
    "### 3. Boosting\n",
    "Boosting is an ensemble technique that focuses on converting weak learners into strong learners by sequentially training models. Each new model attempts to correct the errors made by previous models, placing more weight on misclassified instances.\n",
    "\n",
    "- **Key Points**:\n",
    "  - Reduces both bias and variance.\n",
    "  - Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "### 4. Stacking\n",
    "Stacking is a technique where multiple models (of potentially different types) are trained to predict the same target. A new model, called a meta-learner, is then trained on the predictions of the base models to produce the final output.\n",
    "\n",
    "- **Key Points**:\n",
    "  - Can leverage different types of models (e.g., decision trees, linear models).\n",
    "  - Helps capture complex patterns that single models may miss.\n",
    "\n",
    "### Summary\n",
    "Ensemble learning methods are powerful techniques that can significantly improve model performance by combining the strengths of multiple models. By understanding and applying these techniques, you can enhance the predictive capabilities of your machine learning solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e34f16-1432-4777-b98d-4c99f68c9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Voting, Bagging, Boosting, and Stacking\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample Data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base models\n",
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=2)\n",
    "\n",
    "# Voting Ensemble\n",
    "voting_model = VotingClassifier(estimators=[('dt1', model1), ('dt2', model2)], voting='hard')\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_accuracy = voting_model.score(X_test, y_test)\n",
    "\n",
    "# Bagging Ensemble\n",
    "bagging_model = BaggingClassifier(base_estimator=model1, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_accuracy = bagging_model.score(X_test, y_test)\n",
    "\n",
    "# Boosting Ensemble\n",
    "boosting_model = AdaBoostClassifier(base_estimator=model1, n_estimators=50, random_state=42)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "boosting_accuracy = boosting_model.score(X_test, y_test)\n",
    "\n",
    "# Stacking Ensemble\n",
    "stacking_model = StackingClassifier(estimators=[('dt1', model1), ('dt2', model2)], final_estimator=model1)\n",
    "stacking_model.fit(X_train, y_train)\n",
    "stacking_accuracy = stacking_model.score(X_test, y_test)\n",
    "\n",
    "print(f'Voting Model Accuracy: {voting_accuracy:.2f}')\n",
    "print(f'Bagging Model Accuracy: {bagging_accuracy:.2f}')\n",
    "print(f'Boosting Model Accuracy: {boosting_accuracy:.2f}')\n",
    "print(f'Stacking Model Accuracy: {stacking_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed09e0a-a805-435d-8025-df09cf2f7816",
   "metadata": {},
   "source": [
    "## Imbalanced Data in Machine Learning\n",
    "\n",
    "**Imbalanced Data** refers to situations where the classes in a dataset are not represented equally. This is a common issue in classification tasks, particularly in fraud detection, medical diagnosis, and other applications where one class is rare compared to others. Imbalanced datasets can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "### Key Concepts\n",
    "- **Minority Class**: The class with fewer instances, which is often the class of interest.\n",
    "- **Majority Class**: The class with a larger number of instances.\n",
    "\n",
    "---\n",
    "\n",
    "## Techniques to Handle Imbalanced Data\n",
    "\n",
    "### 1. Undersampling\n",
    "Undersampling involves reducing the number of instances in the majority class to achieve a more balanced dataset. While it can help in mitigating the bias towards the majority class, it may lead to the loss of important information.\n",
    "\n",
    "- **Pros**:\n",
    "  - Reduces training time.\n",
    "  - Prevents overfitting on the majority class.\n",
    "\n",
    "- **Cons**:\n",
    "  - Potential loss of valuable data from the majority class.\n",
    "  \n",
    "### 2. Oversampling\n",
    "Oversampling increases the number of instances in the minority class by duplicating existing samples or generating new ones. This can help balance the dataset but may lead to overfitting since it does not introduce new information.\n",
    "\n",
    "- **Pros**:\n",
    "  - Retains all information from the minority class.\n",
    "  - Can improve model performance on the minority class.\n",
    "\n",
    "- **Cons**:\n",
    "  - May lead to overfitting.\n",
    "\n",
    "### 3. SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "SMOTE is an advanced oversampling technique that generates synthetic samples for the minority class. Instead of duplicating existing samples, it creates new instances by interpolating between existing minority class instances.\n",
    "\n",
    "- **Pros**:\n",
    "  - Helps avoid overfitting by generating new, diverse samples.\n",
    "  - Can improve model performance significantly.\n",
    "\n",
    "- **Cons**:\n",
    "  - Can create noise if not done carefully.\n",
    "  - Requires careful tuning of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Handling imbalanced data is crucial for building effective machine learning models. Techniques like undersampling, oversampling, and SMOTE offer various ways to address this issue, allowing models to learn more effectively from both minority and majority classes. Choosing the right technique depends on the specific context and characteristics of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ef167-d374-4012-8926-94c536516af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Handling Imbalanced Data: Undersampling, Oversampling, and SMOTE\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Generate imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Count original classes\n",
    "print(f'Original dataset shape {Counter(y_train)}')\n",
    "\n",
    "# Undersampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_maj = X_train[y_train == 0]\n",
    "y_maj = y_train[y_train == 0]\n",
    "X_min = X_train[y_train == 1]\n",
    "y_min = y_train[y_train == 1]\n",
    "\n",
    "# Undersample majority class\n",
    "X_maj_undersampled, y_maj_undersampled = resample(X_maj, y_maj,\n",
    "                                                   replace=False,    \n",
    "                                                   n_samples=len(y_min), \n",
    "                                                   random_state=42)\n",
    "\n",
    "# Combine minority class with undersampled majority class\n",
    "X_train_undersampled = pd.concat([pd.DataFrame(X_maj_undersampled), pd.DataFrame(X_min)])\n",
    "y_train_undersampled = pd.concat([pd.Series(y_maj_undersampled), pd.Series(y_min)])\n",
    "\n",
    "print(f'Undersampled dataset shape {Counter(y_train_undersampled)}')\n",
    "\n",
    "# Oversampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Oversample minority class\n",
    "X_min_oversampled, y_min_oversampled = resample(X_min, y_min,\n",
    "                                                replace=True,    \n",
    "                                                n_samples=len(y_maj), \n",
    "                                                random_state=42)\n",
    "\n",
    "# Combine majority class with oversampled minority class\n",
    "X_train_oversampled = pd.concat([pd.DataFrame(X_maj), pd.DataFrame(X_min_oversampled)])\n",
    "y_train_oversampled = pd.concat([pd.Series(y_maj), pd.Series(y_min_oversampled)])\n",
    "\n",
    "print(f'Oversampled dataset shape {Counter(y_train_oversampled)}')\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'SMOTE dataset shape {Counter(y_train_smote)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
